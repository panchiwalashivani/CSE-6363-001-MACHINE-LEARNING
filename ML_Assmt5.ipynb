{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Assmt5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "mNPx8Y0HJlP3"
      },
      "outputs": [],
      "source": [
        "# Professor allowed me to use other small dataset because my laptop is getting stuck while implement Food101 dataset which large.\n",
        "# Dataset - https://www.kaggle.com/datasets/prasunroy/natural-images\n",
        "#https://www.kaggle.com/code/androbomb/using-cnn-to-classify-images-w-pytorch/notebook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image as mp_image\n",
        "import seaborn as sns\n",
        "\n",
        "# Required magic to display matplotlib plots in notebooks\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from torchvision.datasets.utils import download_url"
      ],
      "metadata": {
        "id": "sePOF6qt-j0p"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip archive\\ \\(1\\).zip"
      ],
      "metadata": {
        "id": "K_sroLugP2CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The images are in a folder named 'input/natural-images/natural_images'\n",
        "training_folder_name = 'data/natural_images'\n",
        "\n",
        "# All images are 128x128 pixels\n",
        "img_size = (128,128)\n",
        "\n",
        "# The folder contains a subfolder for each class of shape\n",
        "classes = sorted(os.listdir(training_folder_name))\n",
        "print(classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr8geRtdKF9h",
        "outputId": "b144bbc9-be45-4e17-bf65-f7553aba31fc"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['airplane', 'car', 'cat', 'dog', 'flower', 'fruit', 'motorbike', 'person']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyTorch libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SJA1tY66jRg",
        "outputId": "fda64bd8-391b-4267-cf5b-eb1bbb621e3e"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported - ready to use PyTorch 1.12.1+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# function to resize image\n",
        "def resize_image(src_image, size=(128,128), bg_color=\"white\"): \n",
        "    from PIL import Image, ImageOps \n",
        "    \n",
        "    # resize the image so the longest dimension matches our target size\n",
        "    src_image.thumbnail(size, Image.ANTIALIAS)\n",
        "    \n",
        "    # Create a new square background image\n",
        "    new_image = Image.new(\"RGB\", size, bg_color)\n",
        "    \n",
        "    # Paste the resized image into the center of the square background\n",
        "    new_image.paste(src_image, (int((size[0] - src_image.size[0]) / 2), int((size[1] - src_image.size[1]) / 2)))\n",
        "  \n",
        "    # return the resized image\n",
        "    return new_image"
      ],
      "metadata": {
        "id": "sGrugLVv6jYt"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_folder_name = 'data/natural_images'\n",
        "\n",
        "# New location for the resized images\n",
        "train_folder = '../working/data/natural_images'\n",
        "\n",
        "\n",
        "# Create resized copies of all of the source images\n",
        "size = (128,128)\n",
        "\n",
        "# Create the output folder if it doesn't already exist\n",
        "if os.path.exists(train_folder):\n",
        "    shutil.rmtree(train_folder)\n",
        "\n",
        "# Loop through each subfolder in the input folder\n",
        "print('Transforming images...')\n",
        "for root, folders, files in os.walk(training_folder_name):\n",
        "    for sub_folder in folders:\n",
        "        print('processing folder ' + sub_folder)\n",
        "        # Create a matching subfolder in the output dir\n",
        "        saveFolder = os.path.join(train_folder,sub_folder)\n",
        "        if not os.path.exists(saveFolder):\n",
        "            os.makedirs(saveFolder)\n",
        "        # Loop through the files in the subfolder\n",
        "        file_names = os.listdir(os.path.join(root,sub_folder))\n",
        "        for file_name in file_names:\n",
        "            # Open the file\n",
        "            file_path = os.path.join(root,sub_folder, file_name)\n",
        "            #print(\"reading \" + file_path)\n",
        "            image = Image.open(file_path)\n",
        "            # Create a resized version and save it\n",
        "            resized_image = resize_image(image, size)\n",
        "            saveAs = os.path.join(saveFolder, file_name)\n",
        "            #print(\"writing \" + saveAs)\n",
        "            resized_image.save(saveAs)\n",
        "\n",
        "print('Done.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJH24wx96jeO",
        "outputId": "cde3af27-6b25-4839-ceb5-6063342422c9"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transforming images...\n",
            "processing folder fruit\n",
            "processing folder person\n",
            "processing folder dog\n",
            "processing folder airplane\n",
            "processing folder car\n",
            "processing folder flower\n",
            "processing folder cat\n",
            "processing folder motorbike\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(data_path):\n",
        "    import torch\n",
        "    import torchvision\n",
        "    import torchvision.transforms as transforms\n",
        "    # Load all the images\n",
        "    transformation = transforms.Compose([\n",
        "        # Randomly augment the image data\n",
        "            # Random horizontal flip\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "            # Random vertical flip\n",
        "        transforms.RandomVerticalFlip(0.3),\n",
        "        # transform to tensors\n",
        "        transforms.ToTensor(),\n",
        "        # Normalize the pixel values (in R, G, and B channels)\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # Load all of the images, transforming them\n",
        "    full_dataset = torchvision.datasets.ImageFolder(\n",
        "        root=data_path,\n",
        "        transform=transformation\n",
        "    )\n",
        "    \n",
        "    \n",
        "    # Split into training (70% and testing (30%) datasets)\n",
        "    train_size = int(0.7 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    \n",
        "    # use torch.utils.data.random_split for training/test split\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "    \n",
        "    # define a loader for the training data we can iterate through in 50-image batches\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=50,\n",
        "        num_workers=0,\n",
        "        shuffle=False\n",
        "    )\n",
        "    \n",
        "    # define a loader for the testing data we can iterate through in 50-image batches\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=50,\n",
        "        num_workers=0,\n",
        "        shuffle=False\n",
        "    )\n",
        "        \n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "# Recall that we have resized the images and saved them into\n",
        "train_folder = '../working/data/natural_images'\n",
        "\n",
        "# Get the iterative dataloaders for test and training data\n",
        "train_loader, test_loader = load_dataset(train_folder)\n",
        "batch_size = train_loader.batch_size\n",
        "print(\"Data loaders ready to read\", train_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Nr_EIvU7Acw",
        "outputId": "d602bf78-f7e7-49f8-d80e-6e9c9bf4cfc3"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaders ready to read ../working/data/natural_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a neural net class\n",
        "class Net(nn.Module):\n",
        "    \n",
        "    \n",
        "    # Defining the Constructor\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # In the init function, we define each layer we will use in our model\n",
        "        \n",
        "        # Our images are RGB, so we have input channels = 3. \n",
        "        # We will apply 12 filters in the first convolutional layer\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n",
        "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "        # We in the end apply max pooling with a kernel size of 2\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        \n",
        "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
        "        self.drop = nn.Dropout2d(p=0.2)\n",
        "        \n",
        "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
        "        # This means that our feature tensors are now 32 x 32, and we've generated 24 of them\n",
        "        \n",
        "        # We need to flatten these in order to feed them to a fully-connected layer\n",
        "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # In the forward function, pass the data through the layers we defined in the init function\n",
        "        \n",
        "        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n",
        "        x = F.relu(self.pool(self.conv1(x))) \n",
        "        \n",
        "        # Use a ReLU activation function after layer 2\n",
        "        x = F.relu(self.pool(self.conv2(x)))  \n",
        "        \n",
        "        # Select some features to drop to prevent overfitting (only drop during training)\n",
        "        x = F.dropout(self.drop(x), training=self.training)\n",
        "        \n",
        "        # Flatten\n",
        "        x = x.view(-1, 32 * 32 * 24)\n",
        "        # Feed to fully-connected layer to predict class\n",
        "        x = self.fc(x)\n",
        "        # Return class probabilities via a log_softmax function \n",
        "        return torch.log_softmax(x, dim=1)\n",
        "    \n",
        "device = \"cpu\"\n",
        "if (torch.cuda.is_available()):\n",
        "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
        "    device = \"cuda\"\n",
        "\n",
        "# Create an instance of the model class and allocate it to the device\n",
        "model = Net(num_classes=len(classes)).to(device)\n",
        "\n",
        "print(model)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp13v_wy7Agx",
        "outputId": "e1cce2eb-38f0-45e6-e457-325b02af6fe3"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (drop): Dropout2d(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=24576, out_features=8, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    print(\"Epoch:\", epoch)\n",
        "    # Process the images in batches\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Use the CPU or GPU as appropriate\n",
        "        # Recall that GPU is optimized for the operations we are dealing with\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # Reset the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Push the data forward through the model layers\n",
        "        output = model(data)\n",
        "        \n",
        "        # Get the loss\n",
        "        loss = loss_criteria(output, target)\n",
        "\n",
        "        # Keep a running total\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Print metrics so we see some progress\n",
        "        print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
        "            \n",
        "    # return average loss for the epoch\n",
        "    avg_loss = train_loss / (batch_idx+1)\n",
        "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "DY_ACiZB7XzB"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader):\n",
        "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        batch_count = 0\n",
        "        for data, target in test_loader:\n",
        "            batch_count += 1\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            # Get the predicted classes for this batch\n",
        "            output = model(data)\n",
        "            \n",
        "            # Calculate the loss for this batch\n",
        "            test_loss += loss_criteria(output, target).item()\n",
        "            \n",
        "            # Calculate the accuracy for this batch\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            correct += torch.sum(target==predicted).item()\n",
        "\n",
        "    # Calculate the average loss and total accuracy for this epoch\n",
        "    avg_loss = test_loss / batch_count\n",
        "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        avg_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    # return average loss for the epoch\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "rNVXKpifU3ll"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use an \"Adam\" optimizer to adjust weights\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Specify the loss criteria\n",
        "loss_criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "# Track metrics in these arrays\n",
        "epoch_nums = []\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "\n",
        "# Train over 10 epochs (We restrict to 10 for time issues)\n",
        "epochs = 10\n",
        "print('Training on', device)\n",
        "for epoch in range(1, epochs + 1):\n",
        "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
        "        test_loss = test(model, device, test_loader)\n",
        "        epoch_nums.append(epoch)\n",
        "        training_loss.append(train_loss)\n",
        "        validation_loss.append(test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZGpqy3XU3tf",
        "outputId": "9e544451-fecd-401c-f499-bfaf44139fdd"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cpu\n",
            "Epoch: 1\n",
            "\tTraining batch 1 Loss: 2.096908\n",
            "\tTraining batch 2 Loss: 8.235077\n",
            "\tTraining batch 3 Loss: 3.078918\n",
            "\tTraining batch 4 Loss: 1.751477\n",
            "\tTraining batch 5 Loss: 1.677387\n",
            "\tTraining batch 6 Loss: 1.656644\n",
            "\tTraining batch 7 Loss: 1.850467\n",
            "\tTraining batch 8 Loss: 1.294575\n",
            "\tTraining batch 9 Loss: 1.424000\n",
            "\tTraining batch 10 Loss: 1.468080\n",
            "\tTraining batch 11 Loss: 1.182117\n",
            "\tTraining batch 12 Loss: 0.919348\n",
            "\tTraining batch 13 Loss: 1.215258\n",
            "\tTraining batch 14 Loss: 0.993055\n",
            "\tTraining batch 15 Loss: 1.112877\n",
            "\tTraining batch 16 Loss: 0.975883\n",
            "\tTraining batch 17 Loss: 0.815488\n",
            "\tTraining batch 18 Loss: 0.702881\n",
            "\tTraining batch 19 Loss: 0.737119\n",
            "\tTraining batch 20 Loss: 0.815104\n",
            "\tTraining batch 21 Loss: 1.088435\n",
            "\tTraining batch 22 Loss: 0.769292\n",
            "\tTraining batch 23 Loss: 0.860663\n",
            "\tTraining batch 24 Loss: 1.493040\n",
            "\tTraining batch 25 Loss: 0.835068\n",
            "\tTraining batch 26 Loss: 0.796367\n",
            "\tTraining batch 27 Loss: 0.798620\n",
            "\tTraining batch 28 Loss: 0.920298\n",
            "\tTraining batch 29 Loss: 0.898270\n",
            "\tTraining batch 30 Loss: 0.874346\n",
            "\tTraining batch 31 Loss: 0.873888\n",
            "\tTraining batch 32 Loss: 0.862139\n",
            "\tTraining batch 33 Loss: 1.071897\n",
            "\tTraining batch 34 Loss: 1.240476\n",
            "\tTraining batch 35 Loss: 0.708516\n",
            "\tTraining batch 36 Loss: 1.036494\n",
            "\tTraining batch 37 Loss: 0.889099\n",
            "\tTraining batch 38 Loss: 0.867827\n",
            "\tTraining batch 39 Loss: 1.365843\n",
            "\tTraining batch 40 Loss: 0.782671\n",
            "\tTraining batch 41 Loss: 0.845727\n",
            "\tTraining batch 42 Loss: 0.731029\n",
            "\tTraining batch 43 Loss: 0.556083\n",
            "\tTraining batch 44 Loss: 0.790145\n",
            "\tTraining batch 45 Loss: 1.048180\n",
            "\tTraining batch 46 Loss: 0.792507\n",
            "\tTraining batch 47 Loss: 1.084989\n",
            "\tTraining batch 48 Loss: 0.720626\n",
            "\tTraining batch 49 Loss: 0.688903\n",
            "\tTraining batch 50 Loss: 0.764903\n",
            "\tTraining batch 51 Loss: 1.019928\n",
            "\tTraining batch 52 Loss: 0.789277\n",
            "\tTraining batch 53 Loss: 1.236741\n",
            "\tTraining batch 54 Loss: 0.843436\n",
            "\tTraining batch 55 Loss: 0.904731\n",
            "\tTraining batch 56 Loss: 1.015718\n",
            "\tTraining batch 57 Loss: 0.829140\n",
            "\tTraining batch 58 Loss: 0.591693\n",
            "\tTraining batch 59 Loss: 1.046979\n",
            "\tTraining batch 60 Loss: 0.766835\n",
            "\tTraining batch 61 Loss: 0.923346\n",
            "\tTraining batch 62 Loss: 0.638280\n",
            "\tTraining batch 63 Loss: 0.777614\n",
            "\tTraining batch 64 Loss: 0.779962\n",
            "\tTraining batch 65 Loss: 0.877473\n",
            "\tTraining batch 66 Loss: 0.618945\n",
            "\tTraining batch 67 Loss: 0.634830\n",
            "\tTraining batch 68 Loss: 0.609594\n",
            "\tTraining batch 69 Loss: 0.574301\n",
            "\tTraining batch 70 Loss: 0.720580\n",
            "\tTraining batch 71 Loss: 0.852486\n",
            "\tTraining batch 72 Loss: 0.684057\n",
            "\tTraining batch 73 Loss: 0.956599\n",
            "\tTraining batch 74 Loss: 0.668068\n",
            "\tTraining batch 75 Loss: 0.689438\n",
            "\tTraining batch 76 Loss: 0.610506\n",
            "\tTraining batch 77 Loss: 0.783828\n",
            "\tTraining batch 78 Loss: 0.797204\n",
            "\tTraining batch 79 Loss: 0.860481\n",
            "\tTraining batch 80 Loss: 0.699600\n",
            "\tTraining batch 81 Loss: 0.988014\n",
            "\tTraining batch 82 Loss: 0.541603\n",
            "\tTraining batch 83 Loss: 0.808102\n",
            "\tTraining batch 84 Loss: 0.597231\n",
            "\tTraining batch 85 Loss: 0.661580\n",
            "\tTraining batch 86 Loss: 0.765481\n",
            "\tTraining batch 87 Loss: 0.707091\n",
            "\tTraining batch 88 Loss: 0.811095\n",
            "\tTraining batch 89 Loss: 0.660879\n",
            "\tTraining batch 90 Loss: 0.651337\n",
            "\tTraining batch 91 Loss: 1.153344\n",
            "\tTraining batch 92 Loss: 0.690720\n",
            "\tTraining batch 93 Loss: 0.805927\n",
            "\tTraining batch 94 Loss: 0.727459\n",
            "\tTraining batch 95 Loss: 0.753596\n",
            "\tTraining batch 96 Loss: 0.667139\n",
            "\tTraining batch 97 Loss: 0.878074\n",
            "Training set: Average loss: 1.007519\n",
            "Validation set: Average loss: 0.520287, Accuracy: 1657/2070 (80%)\n",
            "\n",
            "Epoch: 2\n",
            "\tTraining batch 1 Loss: 0.551227\n",
            "\tTraining batch 2 Loss: 0.659428\n",
            "\tTraining batch 3 Loss: 0.866066\n",
            "\tTraining batch 4 Loss: 0.939730\n",
            "\tTraining batch 5 Loss: 0.784310\n",
            "\tTraining batch 6 Loss: 0.527869\n",
            "\tTraining batch 7 Loss: 0.614598\n",
            "\tTraining batch 8 Loss: 0.492926\n",
            "\tTraining batch 9 Loss: 0.527172\n",
            "\tTraining batch 10 Loss: 0.539429\n",
            "\tTraining batch 11 Loss: 0.654739\n",
            "\tTraining batch 12 Loss: 0.596577\n",
            "\tTraining batch 13 Loss: 0.597284\n",
            "\tTraining batch 14 Loss: 0.469411\n",
            "\tTraining batch 15 Loss: 0.650833\n",
            "\tTraining batch 16 Loss: 0.560128\n",
            "\tTraining batch 17 Loss: 0.668591\n",
            "\tTraining batch 18 Loss: 0.676202\n",
            "\tTraining batch 19 Loss: 0.570349\n",
            "\tTraining batch 20 Loss: 0.869362\n",
            "\tTraining batch 21 Loss: 0.502836\n",
            "\tTraining batch 22 Loss: 0.804455\n",
            "\tTraining batch 23 Loss: 0.990435\n",
            "\tTraining batch 24 Loss: 1.073653\n",
            "\tTraining batch 25 Loss: 0.674207\n",
            "\tTraining batch 26 Loss: 0.596143\n",
            "\tTraining batch 27 Loss: 0.534027\n",
            "\tTraining batch 28 Loss: 0.774987\n",
            "\tTraining batch 29 Loss: 0.502827\n",
            "\tTraining batch 30 Loss: 0.909515\n",
            "\tTraining batch 31 Loss: 0.567430\n",
            "\tTraining batch 32 Loss: 0.586857\n",
            "\tTraining batch 33 Loss: 0.596592\n",
            "\tTraining batch 34 Loss: 0.710916\n",
            "\tTraining batch 35 Loss: 0.504837\n",
            "\tTraining batch 36 Loss: 0.814798\n",
            "\tTraining batch 37 Loss: 0.522761\n",
            "\tTraining batch 38 Loss: 0.549448\n",
            "\tTraining batch 39 Loss: 0.908884\n",
            "\tTraining batch 40 Loss: 0.628218\n",
            "\tTraining batch 41 Loss: 0.511293\n",
            "\tTraining batch 42 Loss: 0.557276\n",
            "\tTraining batch 43 Loss: 0.653400\n",
            "\tTraining batch 44 Loss: 0.672493\n",
            "\tTraining batch 45 Loss: 0.567661\n",
            "\tTraining batch 46 Loss: 0.709579\n",
            "\tTraining batch 47 Loss: 0.800029\n",
            "\tTraining batch 48 Loss: 0.722775\n",
            "\tTraining batch 49 Loss: 0.632672\n",
            "\tTraining batch 50 Loss: 0.890296\n",
            "\tTraining batch 51 Loss: 0.701820\n",
            "\tTraining batch 52 Loss: 0.791586\n",
            "\tTraining batch 53 Loss: 0.830433\n",
            "\tTraining batch 54 Loss: 0.889045\n",
            "\tTraining batch 55 Loss: 0.863793\n",
            "\tTraining batch 56 Loss: 0.856634\n",
            "\tTraining batch 57 Loss: 0.619692\n",
            "\tTraining batch 58 Loss: 0.594100\n",
            "\tTraining batch 59 Loss: 0.972444\n",
            "\tTraining batch 60 Loss: 0.633889\n",
            "\tTraining batch 61 Loss: 0.988716\n",
            "\tTraining batch 62 Loss: 0.661845\n",
            "\tTraining batch 63 Loss: 0.826660\n",
            "\tTraining batch 64 Loss: 0.835420\n",
            "\tTraining batch 65 Loss: 0.786101\n",
            "\tTraining batch 66 Loss: 0.613875\n",
            "\tTraining batch 67 Loss: 0.798224\n",
            "\tTraining batch 68 Loss: 0.638451\n",
            "\tTraining batch 69 Loss: 0.973279\n",
            "\tTraining batch 70 Loss: 0.515633\n",
            "\tTraining batch 71 Loss: 0.611064\n",
            "\tTraining batch 72 Loss: 0.930430\n",
            "\tTraining batch 73 Loss: 0.869497\n",
            "\tTraining batch 74 Loss: 0.619374\n",
            "\tTraining batch 75 Loss: 0.754577\n",
            "\tTraining batch 76 Loss: 0.835813\n",
            "\tTraining batch 77 Loss: 0.737727\n",
            "\tTraining batch 78 Loss: 1.046380\n",
            "\tTraining batch 79 Loss: 0.714774\n",
            "\tTraining batch 80 Loss: 0.528809\n",
            "\tTraining batch 81 Loss: 0.877123\n",
            "\tTraining batch 82 Loss: 0.518082\n",
            "\tTraining batch 83 Loss: 0.633049\n",
            "\tTraining batch 84 Loss: 0.543533\n",
            "\tTraining batch 85 Loss: 0.388943\n",
            "\tTraining batch 86 Loss: 0.720404\n",
            "\tTraining batch 87 Loss: 0.661352\n",
            "\tTraining batch 88 Loss: 0.749399\n",
            "\tTraining batch 89 Loss: 0.465018\n",
            "\tTraining batch 90 Loss: 0.608908\n",
            "\tTraining batch 91 Loss: 0.954953\n",
            "\tTraining batch 92 Loss: 0.649711\n",
            "\tTraining batch 93 Loss: 0.818006\n",
            "\tTraining batch 94 Loss: 0.632503\n",
            "\tTraining batch 95 Loss: 0.686442\n",
            "\tTraining batch 96 Loss: 0.631034\n",
            "\tTraining batch 97 Loss: 0.715934\n",
            "Training set: Average loss: 0.696722\n",
            "Validation set: Average loss: 0.493935, Accuracy: 1669/2070 (81%)\n",
            "\n",
            "Epoch: 3\n",
            "\tTraining batch 1 Loss: 0.758091\n",
            "\tTraining batch 2 Loss: 0.429218\n",
            "\tTraining batch 3 Loss: 1.190127\n",
            "\tTraining batch 4 Loss: 0.486180\n",
            "\tTraining batch 5 Loss: 0.502541\n",
            "\tTraining batch 6 Loss: 0.877885\n",
            "\tTraining batch 7 Loss: 0.436623\n",
            "\tTraining batch 8 Loss: 0.783001\n",
            "\tTraining batch 9 Loss: 0.457780\n",
            "\tTraining batch 10 Loss: 0.637374\n",
            "\tTraining batch 11 Loss: 0.762394\n",
            "\tTraining batch 12 Loss: 0.729554\n",
            "\tTraining batch 13 Loss: 0.640504\n",
            "\tTraining batch 14 Loss: 0.550750\n",
            "\tTraining batch 15 Loss: 0.514225\n",
            "\tTraining batch 16 Loss: 0.456137\n",
            "\tTraining batch 17 Loss: 0.602530\n",
            "\tTraining batch 18 Loss: 0.449154\n",
            "\tTraining batch 19 Loss: 0.890028\n",
            "\tTraining batch 20 Loss: 0.885817\n",
            "\tTraining batch 21 Loss: 0.601744\n",
            "\tTraining batch 22 Loss: 0.976324\n",
            "\tTraining batch 23 Loss: 0.898856\n",
            "\tTraining batch 24 Loss: 0.968676\n",
            "\tTraining batch 25 Loss: 0.717862\n",
            "\tTraining batch 26 Loss: 0.602609\n",
            "\tTraining batch 27 Loss: 0.645531\n",
            "\tTraining batch 28 Loss: 1.024470\n",
            "\tTraining batch 29 Loss: 0.762291\n",
            "\tTraining batch 30 Loss: 0.496675\n",
            "\tTraining batch 31 Loss: 0.670671\n",
            "\tTraining batch 32 Loss: 0.900721\n",
            "\tTraining batch 33 Loss: 0.612894\n",
            "\tTraining batch 34 Loss: 0.835556\n",
            "\tTraining batch 35 Loss: 0.800786\n",
            "\tTraining batch 36 Loss: 1.020927\n",
            "\tTraining batch 37 Loss: 0.679241\n",
            "\tTraining batch 38 Loss: 0.493854\n",
            "\tTraining batch 39 Loss: 0.858309\n",
            "\tTraining batch 40 Loss: 0.664532\n",
            "\tTraining batch 41 Loss: 0.745329\n",
            "\tTraining batch 42 Loss: 0.538300\n",
            "\tTraining batch 43 Loss: 0.508665\n",
            "\tTraining batch 44 Loss: 0.710436\n",
            "\tTraining batch 45 Loss: 0.482354\n",
            "\tTraining batch 46 Loss: 0.324485\n",
            "\tTraining batch 47 Loss: 1.210143\n",
            "\tTraining batch 48 Loss: 0.584613\n",
            "\tTraining batch 49 Loss: 0.645826\n",
            "\tTraining batch 50 Loss: 0.714193\n",
            "\tTraining batch 51 Loss: 0.751720\n",
            "\tTraining batch 52 Loss: 0.886101\n",
            "\tTraining batch 53 Loss: 0.711350\n",
            "\tTraining batch 54 Loss: 0.670448\n",
            "\tTraining batch 55 Loss: 0.566284\n",
            "\tTraining batch 56 Loss: 0.805296\n",
            "\tTraining batch 57 Loss: 0.520736\n",
            "\tTraining batch 58 Loss: 0.626383\n",
            "\tTraining batch 59 Loss: 0.689554\n",
            "\tTraining batch 60 Loss: 0.563833\n",
            "\tTraining batch 61 Loss: 0.749735\n",
            "\tTraining batch 62 Loss: 0.625904\n",
            "\tTraining batch 63 Loss: 0.706401\n",
            "\tTraining batch 64 Loss: 0.753091\n",
            "\tTraining batch 65 Loss: 0.768451\n",
            "\tTraining batch 66 Loss: 0.612080\n",
            "\tTraining batch 67 Loss: 0.678263\n",
            "\tTraining batch 68 Loss: 0.471979\n",
            "\tTraining batch 69 Loss: 0.626433\n",
            "\tTraining batch 70 Loss: 0.401993\n",
            "\tTraining batch 71 Loss: 0.909932\n",
            "\tTraining batch 72 Loss: 0.894177\n",
            "\tTraining batch 73 Loss: 0.734366\n",
            "\tTraining batch 74 Loss: 0.910932\n",
            "\tTraining batch 75 Loss: 0.832375\n",
            "\tTraining batch 76 Loss: 0.780226\n",
            "\tTraining batch 77 Loss: 0.612003\n",
            "\tTraining batch 78 Loss: 0.585602\n",
            "\tTraining batch 79 Loss: 0.616331\n",
            "\tTraining batch 80 Loss: 0.628857\n",
            "\tTraining batch 81 Loss: 0.955826\n",
            "\tTraining batch 82 Loss: 0.644080\n",
            "\tTraining batch 83 Loss: 0.699294\n",
            "\tTraining batch 84 Loss: 0.584999\n",
            "\tTraining batch 85 Loss: 0.468846\n",
            "\tTraining batch 86 Loss: 0.516862\n",
            "\tTraining batch 87 Loss: 0.712861\n",
            "\tTraining batch 88 Loss: 0.573094\n",
            "\tTraining batch 89 Loss: 0.541468\n",
            "\tTraining batch 90 Loss: 0.630600\n",
            "\tTraining batch 91 Loss: 0.476993\n",
            "\tTraining batch 92 Loss: 0.429364\n",
            "\tTraining batch 93 Loss: 0.766610\n",
            "\tTraining batch 94 Loss: 0.764928\n",
            "\tTraining batch 95 Loss: 0.360468\n",
            "\tTraining batch 96 Loss: 0.473719\n",
            "\tTraining batch 97 Loss: 0.452604\n",
            "Training set: Average loss: 0.675054\n",
            "Validation set: Average loss: 0.394496, Accuracy: 1755/2070 (85%)\n",
            "\n",
            "Epoch: 4\n",
            "\tTraining batch 1 Loss: 0.450368\n",
            "\tTraining batch 2 Loss: 0.312527\n",
            "\tTraining batch 3 Loss: 0.878533\n",
            "\tTraining batch 4 Loss: 0.760558\n",
            "\tTraining batch 5 Loss: 0.467533\n",
            "\tTraining batch 6 Loss: 0.367207\n",
            "\tTraining batch 7 Loss: 0.485334\n",
            "\tTraining batch 8 Loss: 0.593964\n",
            "\tTraining batch 9 Loss: 0.243563\n",
            "\tTraining batch 10 Loss: 0.451740\n",
            "\tTraining batch 11 Loss: 0.503321\n",
            "\tTraining batch 12 Loss: 0.563883\n",
            "\tTraining batch 13 Loss: 0.657409\n",
            "\tTraining batch 14 Loss: 0.343911\n",
            "\tTraining batch 15 Loss: 0.416675\n",
            "\tTraining batch 16 Loss: 0.515796\n",
            "\tTraining batch 17 Loss: 0.470921\n",
            "\tTraining batch 18 Loss: 0.669938\n",
            "\tTraining batch 19 Loss: 0.438682\n",
            "\tTraining batch 20 Loss: 0.950038\n",
            "\tTraining batch 21 Loss: 0.506229\n",
            "\tTraining batch 22 Loss: 0.724789\n",
            "\tTraining batch 23 Loss: 0.818671\n",
            "\tTraining batch 24 Loss: 0.986634\n",
            "\tTraining batch 25 Loss: 0.386701\n",
            "\tTraining batch 26 Loss: 0.615891\n",
            "\tTraining batch 27 Loss: 0.525772\n",
            "\tTraining batch 28 Loss: 0.712160\n",
            "\tTraining batch 29 Loss: 0.342167\n",
            "\tTraining batch 30 Loss: 0.726737\n",
            "\tTraining batch 31 Loss: 0.614308\n",
            "\tTraining batch 32 Loss: 0.528200\n",
            "\tTraining batch 33 Loss: 0.463294\n",
            "\tTraining batch 34 Loss: 0.475705\n",
            "\tTraining batch 35 Loss: 0.526824\n",
            "\tTraining batch 36 Loss: 0.867803\n",
            "\tTraining batch 37 Loss: 0.625426\n",
            "\tTraining batch 38 Loss: 0.327283\n",
            "\tTraining batch 39 Loss: 0.657341\n",
            "\tTraining batch 40 Loss: 0.435795\n",
            "\tTraining batch 41 Loss: 0.704080\n",
            "\tTraining batch 42 Loss: 0.437998\n",
            "\tTraining batch 43 Loss: 0.453877\n",
            "\tTraining batch 44 Loss: 0.273821\n",
            "\tTraining batch 45 Loss: 0.495053\n",
            "\tTraining batch 46 Loss: 0.683149\n",
            "\tTraining batch 47 Loss: 1.032111\n",
            "\tTraining batch 48 Loss: 0.701153\n",
            "\tTraining batch 49 Loss: 0.464550\n",
            "\tTraining batch 50 Loss: 0.396738\n",
            "\tTraining batch 51 Loss: 0.838717\n",
            "\tTraining batch 52 Loss: 0.637136\n",
            "\tTraining batch 53 Loss: 0.768825\n",
            "\tTraining batch 54 Loss: 0.600306\n",
            "\tTraining batch 55 Loss: 0.517442\n",
            "\tTraining batch 56 Loss: 0.486465\n",
            "\tTraining batch 57 Loss: 0.863599\n",
            "\tTraining batch 58 Loss: 0.343469\n",
            "\tTraining batch 59 Loss: 0.742929\n",
            "\tTraining batch 60 Loss: 0.760741\n",
            "\tTraining batch 61 Loss: 0.643431\n",
            "\tTraining batch 62 Loss: 0.459889\n",
            "\tTraining batch 63 Loss: 0.593825\n",
            "\tTraining batch 64 Loss: 0.377027\n",
            "\tTraining batch 65 Loss: 1.006211\n",
            "\tTraining batch 66 Loss: 0.523178\n",
            "\tTraining batch 67 Loss: 0.424799\n",
            "\tTraining batch 68 Loss: 0.398362\n",
            "\tTraining batch 69 Loss: 0.747710\n",
            "\tTraining batch 70 Loss: 0.515735\n",
            "\tTraining batch 71 Loss: 0.766728\n",
            "\tTraining batch 72 Loss: 0.714910\n",
            "\tTraining batch 73 Loss: 0.507667\n",
            "\tTraining batch 74 Loss: 0.675841\n",
            "\tTraining batch 75 Loss: 0.671051\n",
            "\tTraining batch 76 Loss: 0.541816\n",
            "\tTraining batch 77 Loss: 0.455705\n",
            "\tTraining batch 78 Loss: 0.563154\n",
            "\tTraining batch 79 Loss: 0.579785\n",
            "\tTraining batch 80 Loss: 0.543314\n",
            "\tTraining batch 81 Loss: 0.631395\n",
            "\tTraining batch 82 Loss: 0.262094\n",
            "\tTraining batch 83 Loss: 0.674676\n",
            "\tTraining batch 84 Loss: 0.345658\n",
            "\tTraining batch 85 Loss: 0.436158\n",
            "\tTraining batch 86 Loss: 0.301343\n",
            "\tTraining batch 87 Loss: 0.621793\n",
            "\tTraining batch 88 Loss: 0.438054\n",
            "\tTraining batch 89 Loss: 0.650954\n",
            "\tTraining batch 90 Loss: 0.328894\n",
            "\tTraining batch 91 Loss: 0.579224\n",
            "\tTraining batch 92 Loss: 0.269102\n",
            "\tTraining batch 93 Loss: 0.511448\n",
            "\tTraining batch 94 Loss: 0.495564\n",
            "\tTraining batch 95 Loss: 0.464294\n",
            "\tTraining batch 96 Loss: 0.438179\n",
            "\tTraining batch 97 Loss: 0.577043\n",
            "Training set: Average loss: 0.560287\n",
            "Validation set: Average loss: 0.415708, Accuracy: 1746/2070 (84%)\n",
            "\n",
            "Epoch: 5\n",
            "\tTraining batch 1 Loss: 0.371463\n",
            "\tTraining batch 2 Loss: 0.346514\n",
            "\tTraining batch 3 Loss: 0.739828\n",
            "\tTraining batch 4 Loss: 0.506766\n",
            "\tTraining batch 5 Loss: 0.409160\n",
            "\tTraining batch 6 Loss: 0.556407\n",
            "\tTraining batch 7 Loss: 0.412242\n",
            "\tTraining batch 8 Loss: 0.668649\n",
            "\tTraining batch 9 Loss: 0.413727\n",
            "\tTraining batch 10 Loss: 0.533775\n",
            "\tTraining batch 11 Loss: 0.395133\n",
            "\tTraining batch 12 Loss: 0.691875\n",
            "\tTraining batch 13 Loss: 0.307772\n",
            "\tTraining batch 14 Loss: 0.693619\n",
            "\tTraining batch 15 Loss: 0.608804\n",
            "\tTraining batch 16 Loss: 0.459308\n",
            "\tTraining batch 17 Loss: 0.596163\n",
            "\tTraining batch 18 Loss: 0.484087\n",
            "\tTraining batch 19 Loss: 0.454298\n",
            "\tTraining batch 20 Loss: 0.771631\n",
            "\tTraining batch 21 Loss: 0.212535\n",
            "\tTraining batch 22 Loss: 0.523408\n",
            "\tTraining batch 23 Loss: 0.471496\n",
            "\tTraining batch 24 Loss: 0.608310\n",
            "\tTraining batch 25 Loss: 0.606006\n",
            "\tTraining batch 26 Loss: 0.488349\n",
            "\tTraining batch 27 Loss: 0.576528\n",
            "\tTraining batch 28 Loss: 0.593123\n",
            "\tTraining batch 29 Loss: 0.357300\n",
            "\tTraining batch 30 Loss: 0.421042\n",
            "\tTraining batch 31 Loss: 0.520225\n",
            "\tTraining batch 32 Loss: 0.596007\n",
            "\tTraining batch 33 Loss: 0.556600\n",
            "\tTraining batch 34 Loss: 0.494909\n",
            "\tTraining batch 35 Loss: 0.434891\n",
            "\tTraining batch 36 Loss: 0.536374\n",
            "\tTraining batch 37 Loss: 0.545779\n",
            "\tTraining batch 38 Loss: 0.249509\n",
            "\tTraining batch 39 Loss: 0.560226\n",
            "\tTraining batch 40 Loss: 0.449177\n",
            "\tTraining batch 41 Loss: 0.429178\n",
            "\tTraining batch 42 Loss: 0.399580\n",
            "\tTraining batch 43 Loss: 0.260839\n",
            "\tTraining batch 44 Loss: 0.269289\n",
            "\tTraining batch 45 Loss: 0.674407\n",
            "\tTraining batch 46 Loss: 0.813356\n",
            "\tTraining batch 47 Loss: 1.048354\n",
            "\tTraining batch 48 Loss: 0.454402\n",
            "\tTraining batch 49 Loss: 0.496370\n",
            "\tTraining batch 50 Loss: 0.471390\n",
            "\tTraining batch 51 Loss: 0.475850\n",
            "\tTraining batch 52 Loss: 0.986239\n",
            "\tTraining batch 53 Loss: 0.588336\n",
            "\tTraining batch 54 Loss: 0.467354\n",
            "\tTraining batch 55 Loss: 0.290246\n",
            "\tTraining batch 56 Loss: 0.560033\n",
            "\tTraining batch 57 Loss: 0.478329\n",
            "\tTraining batch 58 Loss: 0.365785\n",
            "\tTraining batch 59 Loss: 0.548153\n",
            "\tTraining batch 60 Loss: 0.370547\n",
            "\tTraining batch 61 Loss: 0.669794\n",
            "\tTraining batch 62 Loss: 0.379593\n",
            "\tTraining batch 63 Loss: 0.627177\n",
            "\tTraining batch 64 Loss: 0.458612\n",
            "\tTraining batch 65 Loss: 0.387248\n",
            "\tTraining batch 66 Loss: 0.486685\n",
            "\tTraining batch 67 Loss: 0.497607\n",
            "\tTraining batch 68 Loss: 0.559211\n",
            "\tTraining batch 69 Loss: 0.302678\n",
            "\tTraining batch 70 Loss: 0.327464\n",
            "\tTraining batch 71 Loss: 0.653974\n",
            "\tTraining batch 72 Loss: 0.729093\n",
            "\tTraining batch 73 Loss: 0.613748\n",
            "\tTraining batch 74 Loss: 0.409359\n",
            "\tTraining batch 75 Loss: 0.364489\n",
            "\tTraining batch 76 Loss: 0.559150\n",
            "\tTraining batch 77 Loss: 0.520864\n",
            "\tTraining batch 78 Loss: 0.572295\n",
            "\tTraining batch 79 Loss: 0.548798\n",
            "\tTraining batch 80 Loss: 0.445406\n",
            "\tTraining batch 81 Loss: 0.662066\n",
            "\tTraining batch 82 Loss: 0.364982\n",
            "\tTraining batch 83 Loss: 0.641534\n",
            "\tTraining batch 84 Loss: 0.337342\n",
            "\tTraining batch 85 Loss: 0.368203\n",
            "\tTraining batch 86 Loss: 0.377171\n",
            "\tTraining batch 87 Loss: 0.466647\n",
            "\tTraining batch 88 Loss: 0.276826\n",
            "\tTraining batch 89 Loss: 0.521888\n",
            "\tTraining batch 90 Loss: 0.637027\n",
            "\tTraining batch 91 Loss: 0.492090\n",
            "\tTraining batch 92 Loss: 0.309385\n",
            "\tTraining batch 93 Loss: 0.560646\n",
            "\tTraining batch 94 Loss: 0.446059\n",
            "\tTraining batch 95 Loss: 0.341221\n",
            "\tTraining batch 96 Loss: 0.416820\n",
            "\tTraining batch 97 Loss: 0.332755\n",
            "Training set: Average loss: 0.498319\n",
            "Validation set: Average loss: 0.365528, Accuracy: 1758/2070 (85%)\n",
            "\n",
            "Epoch: 6\n",
            "\tTraining batch 1 Loss: 0.460667\n",
            "\tTraining batch 2 Loss: 0.473341\n",
            "\tTraining batch 3 Loss: 0.584468\n",
            "\tTraining batch 4 Loss: 0.436120\n",
            "\tTraining batch 5 Loss: 0.531994\n",
            "\tTraining batch 6 Loss: 0.356539\n",
            "\tTraining batch 7 Loss: 0.483155\n",
            "\tTraining batch 8 Loss: 0.521659\n",
            "\tTraining batch 9 Loss: 0.312887\n",
            "\tTraining batch 10 Loss: 0.373037\n",
            "\tTraining batch 11 Loss: 0.393051\n",
            "\tTraining batch 12 Loss: 0.333013\n",
            "\tTraining batch 13 Loss: 0.298797\n",
            "\tTraining batch 14 Loss: 0.283350\n",
            "\tTraining batch 15 Loss: 0.458988\n",
            "\tTraining batch 16 Loss: 0.354283\n",
            "\tTraining batch 17 Loss: 0.618549\n",
            "\tTraining batch 18 Loss: 0.403062\n",
            "\tTraining batch 19 Loss: 0.727647\n",
            "\tTraining batch 20 Loss: 0.800388\n",
            "\tTraining batch 21 Loss: 0.288750\n",
            "\tTraining batch 22 Loss: 0.424804\n",
            "\tTraining batch 23 Loss: 0.371028\n",
            "\tTraining batch 24 Loss: 0.380071\n",
            "\tTraining batch 25 Loss: 0.391085\n",
            "\tTraining batch 26 Loss: 0.720531\n",
            "\tTraining batch 27 Loss: 0.572642\n",
            "\tTraining batch 28 Loss: 0.259678\n",
            "\tTraining batch 29 Loss: 0.281291\n",
            "\tTraining batch 30 Loss: 0.246934\n",
            "\tTraining batch 31 Loss: 0.384235\n",
            "\tTraining batch 32 Loss: 0.384120\n",
            "\tTraining batch 33 Loss: 0.466052\n",
            "\tTraining batch 34 Loss: 0.544857\n",
            "\tTraining batch 35 Loss: 0.389663\n",
            "\tTraining batch 36 Loss: 0.546269\n",
            "\tTraining batch 37 Loss: 0.527021\n",
            "\tTraining batch 38 Loss: 0.218678\n",
            "\tTraining batch 39 Loss: 0.512254\n",
            "\tTraining batch 40 Loss: 0.402499\n",
            "\tTraining batch 41 Loss: 0.402735\n",
            "\tTraining batch 42 Loss: 0.314486\n",
            "\tTraining batch 43 Loss: 0.381732\n",
            "\tTraining batch 44 Loss: 0.252108\n",
            "\tTraining batch 45 Loss: 0.588741\n",
            "\tTraining batch 46 Loss: 0.226929\n",
            "\tTraining batch 47 Loss: 0.568394\n",
            "\tTraining batch 48 Loss: 0.507211\n",
            "\tTraining batch 49 Loss: 0.534042\n",
            "\tTraining batch 50 Loss: 0.336604\n",
            "\tTraining batch 51 Loss: 0.371851\n",
            "\tTraining batch 52 Loss: 0.370231\n",
            "\tTraining batch 53 Loss: 0.666378\n",
            "\tTraining batch 54 Loss: 0.427468\n",
            "\tTraining batch 55 Loss: 1.113899\n",
            "\tTraining batch 56 Loss: 0.355300\n",
            "\tTraining batch 57 Loss: 0.469314\n",
            "\tTraining batch 58 Loss: 0.370741\n",
            "\tTraining batch 59 Loss: 0.600429\n",
            "\tTraining batch 60 Loss: 0.718654\n",
            "\tTraining batch 61 Loss: 0.543997\n",
            "\tTraining batch 62 Loss: 0.309566\n",
            "\tTraining batch 63 Loss: 0.393061\n",
            "\tTraining batch 64 Loss: 0.528945\n",
            "\tTraining batch 65 Loss: 0.407933\n",
            "\tTraining batch 66 Loss: 0.549702\n",
            "\tTraining batch 67 Loss: 0.549860\n",
            "\tTraining batch 68 Loss: 0.441551\n",
            "\tTraining batch 69 Loss: 0.503284\n",
            "\tTraining batch 70 Loss: 0.435920\n",
            "\tTraining batch 71 Loss: 0.389589\n",
            "\tTraining batch 72 Loss: 0.928800\n",
            "\tTraining batch 73 Loss: 0.808248\n",
            "\tTraining batch 74 Loss: 0.584210\n",
            "\tTraining batch 75 Loss: 0.524260\n",
            "\tTraining batch 76 Loss: 0.335744\n",
            "\tTraining batch 77 Loss: 0.572126\n",
            "\tTraining batch 78 Loss: 0.534293\n",
            "\tTraining batch 79 Loss: 0.675315\n",
            "\tTraining batch 80 Loss: 0.301959\n",
            "\tTraining batch 81 Loss: 0.607233\n",
            "\tTraining batch 82 Loss: 0.296065\n",
            "\tTraining batch 83 Loss: 0.257260\n",
            "\tTraining batch 84 Loss: 0.269093\n",
            "\tTraining batch 85 Loss: 0.227353\n",
            "\tTraining batch 86 Loss: 0.513836\n",
            "\tTraining batch 87 Loss: 1.039408\n",
            "\tTraining batch 88 Loss: 0.329244\n",
            "\tTraining batch 89 Loss: 0.363842\n",
            "\tTraining batch 90 Loss: 0.187007\n",
            "\tTraining batch 91 Loss: 0.313820\n",
            "\tTraining batch 92 Loss: 0.174899\n",
            "\tTraining batch 93 Loss: 0.547812\n",
            "\tTraining batch 94 Loss: 0.717915\n",
            "\tTraining batch 95 Loss: 0.517857\n",
            "\tTraining batch 96 Loss: 0.282567\n",
            "\tTraining batch 97 Loss: 0.376479\n",
            "Training set: Average loss: 0.458090\n",
            "Validation set: Average loss: 0.386505, Accuracy: 1775/2070 (86%)\n",
            "\n",
            "Epoch: 7\n",
            "\tTraining batch 1 Loss: 0.220773\n",
            "\tTraining batch 2 Loss: 0.432013\n",
            "\tTraining batch 3 Loss: 0.752184\n",
            "\tTraining batch 4 Loss: 0.605765\n",
            "\tTraining batch 5 Loss: 0.278186\n",
            "\tTraining batch 6 Loss: 0.356964\n",
            "\tTraining batch 7 Loss: 0.357295\n",
            "\tTraining batch 8 Loss: 0.452498\n",
            "\tTraining batch 9 Loss: 0.168548\n",
            "\tTraining batch 10 Loss: 0.419575\n",
            "\tTraining batch 11 Loss: 0.247760\n",
            "\tTraining batch 12 Loss: 0.244953\n",
            "\tTraining batch 13 Loss: 0.411638\n",
            "\tTraining batch 14 Loss: 0.642947\n",
            "\tTraining batch 15 Loss: 0.371438\n",
            "\tTraining batch 16 Loss: 0.245694\n",
            "\tTraining batch 17 Loss: 0.600975\n",
            "\tTraining batch 18 Loss: 0.309786\n",
            "\tTraining batch 19 Loss: 0.532971\n",
            "\tTraining batch 20 Loss: 0.441099\n",
            "\tTraining batch 21 Loss: 0.111595\n",
            "\tTraining batch 22 Loss: 0.596410\n",
            "\tTraining batch 23 Loss: 0.618997\n",
            "\tTraining batch 24 Loss: 0.489640\n",
            "\tTraining batch 25 Loss: 0.405252\n",
            "\tTraining batch 26 Loss: 0.345734\n",
            "\tTraining batch 27 Loss: 0.391580\n",
            "\tTraining batch 28 Loss: 0.386155\n",
            "\tTraining batch 29 Loss: 0.251994\n",
            "\tTraining batch 30 Loss: 0.373772\n",
            "\tTraining batch 31 Loss: 0.400540\n",
            "\tTraining batch 32 Loss: 0.350854\n",
            "\tTraining batch 33 Loss: 0.437597\n",
            "\tTraining batch 34 Loss: 0.400510\n",
            "\tTraining batch 35 Loss: 0.524340\n",
            "\tTraining batch 36 Loss: 0.463914\n",
            "\tTraining batch 37 Loss: 0.321912\n",
            "\tTraining batch 38 Loss: 0.133573\n",
            "\tTraining batch 39 Loss: 0.772736\n",
            "\tTraining batch 40 Loss: 0.508028\n",
            "\tTraining batch 41 Loss: 0.631968\n",
            "\tTraining batch 42 Loss: 0.239874\n",
            "\tTraining batch 43 Loss: 0.275508\n",
            "\tTraining batch 44 Loss: 0.252240\n",
            "\tTraining batch 45 Loss: 0.360709\n",
            "\tTraining batch 46 Loss: 0.657538\n",
            "\tTraining batch 47 Loss: 0.857689\n",
            "\tTraining batch 48 Loss: 0.334204\n",
            "\tTraining batch 49 Loss: 0.374152\n",
            "\tTraining batch 50 Loss: 0.515456\n",
            "\tTraining batch 51 Loss: 0.331095\n",
            "\tTraining batch 52 Loss: 0.296493\n",
            "\tTraining batch 53 Loss: 0.488093\n",
            "\tTraining batch 54 Loss: 0.487085\n",
            "\tTraining batch 55 Loss: 0.974920\n",
            "\tTraining batch 56 Loss: 0.358396\n",
            "\tTraining batch 57 Loss: 0.369961\n",
            "\tTraining batch 58 Loss: 0.267383\n",
            "\tTraining batch 59 Loss: 0.512834\n",
            "\tTraining batch 60 Loss: 0.553662\n",
            "\tTraining batch 61 Loss: 0.583551\n",
            "\tTraining batch 62 Loss: 0.379166\n",
            "\tTraining batch 63 Loss: 0.456052\n",
            "\tTraining batch 64 Loss: 0.528813\n",
            "\tTraining batch 65 Loss: 0.462037\n",
            "\tTraining batch 66 Loss: 0.339065\n",
            "\tTraining batch 67 Loss: 0.599897\n",
            "\tTraining batch 68 Loss: 0.349220\n",
            "\tTraining batch 69 Loss: 0.517418\n",
            "\tTraining batch 70 Loss: 0.292188\n",
            "\tTraining batch 71 Loss: 0.585769\n",
            "\tTraining batch 72 Loss: 0.604114\n",
            "\tTraining batch 73 Loss: 0.402846\n",
            "\tTraining batch 74 Loss: 0.376579\n",
            "\tTraining batch 75 Loss: 0.658696\n",
            "\tTraining batch 76 Loss: 0.356870\n",
            "\tTraining batch 77 Loss: 0.374861\n",
            "\tTraining batch 78 Loss: 0.367081\n",
            "\tTraining batch 79 Loss: 0.636012\n",
            "\tTraining batch 80 Loss: 0.221194\n",
            "\tTraining batch 81 Loss: 0.535663\n",
            "\tTraining batch 82 Loss: 0.329851\n",
            "\tTraining batch 83 Loss: 0.434300\n",
            "\tTraining batch 84 Loss: 0.213641\n",
            "\tTraining batch 85 Loss: 0.205994\n",
            "\tTraining batch 86 Loss: 0.433933\n",
            "\tTraining batch 87 Loss: 0.321489\n",
            "\tTraining batch 88 Loss: 0.375000\n",
            "\tTraining batch 89 Loss: 0.467965\n",
            "\tTraining batch 90 Loss: 0.401062\n",
            "\tTraining batch 91 Loss: 0.608966\n",
            "\tTraining batch 92 Loss: 0.333163\n",
            "\tTraining batch 93 Loss: 0.449496\n",
            "\tTraining batch 94 Loss: 0.294702\n",
            "\tTraining batch 95 Loss: 0.266675\n",
            "\tTraining batch 96 Loss: 0.372867\n",
            "\tTraining batch 97 Loss: 0.510871\n",
            "Training set: Average loss: 0.424377\n",
            "Validation set: Average loss: 0.361170, Accuracy: 1771/2070 (86%)\n",
            "\n",
            "Epoch: 8\n",
            "\tTraining batch 1 Loss: 0.359571\n",
            "\tTraining batch 2 Loss: 0.308965\n",
            "\tTraining batch 3 Loss: 0.703467\n",
            "\tTraining batch 4 Loss: 0.459437\n",
            "\tTraining batch 5 Loss: 0.448230\n",
            "\tTraining batch 6 Loss: 0.390807\n",
            "\tTraining batch 7 Loss: 0.402168\n",
            "\tTraining batch 8 Loss: 0.551141\n",
            "\tTraining batch 9 Loss: 0.248593\n",
            "\tTraining batch 10 Loss: 0.263447\n",
            "\tTraining batch 11 Loss: 0.377971\n",
            "\tTraining batch 12 Loss: 0.353829\n",
            "\tTraining batch 13 Loss: 0.374785\n",
            "\tTraining batch 14 Loss: 0.417482\n",
            "\tTraining batch 15 Loss: 0.380418\n",
            "\tTraining batch 16 Loss: 0.287214\n",
            "\tTraining batch 17 Loss: 0.425080\n",
            "\tTraining batch 18 Loss: 0.347049\n",
            "\tTraining batch 19 Loss: 0.328327\n",
            "\tTraining batch 20 Loss: 0.544833\n",
            "\tTraining batch 21 Loss: 0.212809\n",
            "\tTraining batch 22 Loss: 0.973151\n",
            "\tTraining batch 23 Loss: 0.353726\n",
            "\tTraining batch 24 Loss: 0.688415\n",
            "\tTraining batch 25 Loss: 0.339108\n",
            "\tTraining batch 26 Loss: 0.291632\n",
            "\tTraining batch 27 Loss: 0.462689\n",
            "\tTraining batch 28 Loss: 0.637497\n",
            "\tTraining batch 29 Loss: 0.304036\n",
            "\tTraining batch 30 Loss: 0.321382\n",
            "\tTraining batch 31 Loss: 0.581972\n",
            "\tTraining batch 32 Loss: 0.464863\n",
            "\tTraining batch 33 Loss: 0.306111\n",
            "\tTraining batch 34 Loss: 0.335743\n",
            "\tTraining batch 35 Loss: 0.442089\n",
            "\tTraining batch 36 Loss: 1.042238\n",
            "\tTraining batch 37 Loss: 0.295340\n",
            "\tTraining batch 38 Loss: 0.172803\n",
            "\tTraining batch 39 Loss: 0.443724\n",
            "\tTraining batch 40 Loss: 0.244892\n",
            "\tTraining batch 41 Loss: 0.270282\n",
            "\tTraining batch 42 Loss: 0.279351\n",
            "\tTraining batch 43 Loss: 0.442462\n",
            "\tTraining batch 44 Loss: 0.298403\n",
            "\tTraining batch 45 Loss: 0.440484\n",
            "\tTraining batch 46 Loss: 0.579507\n",
            "\tTraining batch 47 Loss: 0.658056\n",
            "\tTraining batch 48 Loss: 0.384535\n",
            "\tTraining batch 49 Loss: 0.300901\n",
            "\tTraining batch 50 Loss: 0.489854\n",
            "\tTraining batch 51 Loss: 0.494451\n",
            "\tTraining batch 52 Loss: 0.490891\n",
            "\tTraining batch 53 Loss: 0.528038\n",
            "\tTraining batch 54 Loss: 0.299774\n",
            "\tTraining batch 55 Loss: 0.362036\n",
            "\tTraining batch 56 Loss: 0.582103\n",
            "\tTraining batch 57 Loss: 0.511919\n",
            "\tTraining batch 58 Loss: 0.105508\n",
            "\tTraining batch 59 Loss: 0.653118\n",
            "\tTraining batch 60 Loss: 0.460879\n",
            "\tTraining batch 61 Loss: 0.575058\n",
            "\tTraining batch 62 Loss: 0.223369\n",
            "\tTraining batch 63 Loss: 0.349803\n",
            "\tTraining batch 64 Loss: 0.335434\n",
            "\tTraining batch 65 Loss: 0.386310\n",
            "\tTraining batch 66 Loss: 0.401899\n",
            "\tTraining batch 67 Loss: 0.346026\n",
            "\tTraining batch 68 Loss: 0.320726\n",
            "\tTraining batch 69 Loss: 0.319235\n",
            "\tTraining batch 70 Loss: 0.462024\n",
            "\tTraining batch 71 Loss: 0.336605\n",
            "\tTraining batch 72 Loss: 0.353872\n",
            "\tTraining batch 73 Loss: 0.556589\n",
            "\tTraining batch 74 Loss: 0.528541\n",
            "\tTraining batch 75 Loss: 0.429221\n",
            "\tTraining batch 76 Loss: 0.428247\n",
            "\tTraining batch 77 Loss: 0.521627\n",
            "\tTraining batch 78 Loss: 0.575574\n",
            "\tTraining batch 79 Loss: 0.341678\n",
            "\tTraining batch 80 Loss: 0.286779\n",
            "\tTraining batch 81 Loss: 0.495130\n",
            "\tTraining batch 82 Loss: 0.219263\n",
            "\tTraining batch 83 Loss: 0.506440\n",
            "\tTraining batch 84 Loss: 0.193022\n",
            "\tTraining batch 85 Loss: 0.319171\n",
            "\tTraining batch 86 Loss: 0.317978\n",
            "\tTraining batch 87 Loss: 0.243284\n",
            "\tTraining batch 88 Loss: 0.249078\n",
            "\tTraining batch 89 Loss: 0.372225\n",
            "\tTraining batch 90 Loss: 0.368187\n",
            "\tTraining batch 91 Loss: 0.418036\n",
            "\tTraining batch 92 Loss: 0.187655\n",
            "\tTraining batch 93 Loss: 0.444094\n",
            "\tTraining batch 94 Loss: 0.229179\n",
            "\tTraining batch 95 Loss: 0.354399\n",
            "\tTraining batch 96 Loss: 0.262100\n",
            "\tTraining batch 97 Loss: 0.362127\n",
            "Training set: Average loss: 0.403501\n",
            "Validation set: Average loss: 0.329003, Accuracy: 1803/2070 (87%)\n",
            "\n",
            "Epoch: 9\n",
            "\tTraining batch 1 Loss: 0.328370\n",
            "\tTraining batch 2 Loss: 0.385380\n",
            "\tTraining batch 3 Loss: 0.399111\n",
            "\tTraining batch 4 Loss: 0.221235\n",
            "\tTraining batch 5 Loss: 0.276320\n",
            "\tTraining batch 6 Loss: 0.338583\n",
            "\tTraining batch 7 Loss: 0.329031\n",
            "\tTraining batch 8 Loss: 0.499726\n",
            "\tTraining batch 9 Loss: 0.384824\n",
            "\tTraining batch 10 Loss: 0.445010\n",
            "\tTraining batch 11 Loss: 0.375873\n",
            "\tTraining batch 12 Loss: 0.308767\n",
            "\tTraining batch 13 Loss: 0.340399\n",
            "\tTraining batch 14 Loss: 0.228074\n",
            "\tTraining batch 15 Loss: 0.403955\n",
            "\tTraining batch 16 Loss: 0.272247\n",
            "\tTraining batch 17 Loss: 0.510940\n",
            "\tTraining batch 18 Loss: 0.345082\n",
            "\tTraining batch 19 Loss: 0.482328\n",
            "\tTraining batch 20 Loss: 1.100607\n",
            "\tTraining batch 21 Loss: 0.279462\n",
            "\tTraining batch 22 Loss: 0.528097\n",
            "\tTraining batch 23 Loss: 0.583334\n",
            "\tTraining batch 24 Loss: 0.470102\n",
            "\tTraining batch 25 Loss: 0.471630\n",
            "\tTraining batch 26 Loss: 0.580394\n",
            "\tTraining batch 27 Loss: 0.458620\n",
            "\tTraining batch 28 Loss: 0.412148\n",
            "\tTraining batch 29 Loss: 0.345564\n",
            "\tTraining batch 30 Loss: 0.412630\n",
            "\tTraining batch 31 Loss: 0.601225\n",
            "\tTraining batch 32 Loss: 0.331332\n",
            "\tTraining batch 33 Loss: 0.367373\n",
            "\tTraining batch 34 Loss: 0.321432\n",
            "\tTraining batch 35 Loss: 0.331137\n",
            "\tTraining batch 36 Loss: 0.524570\n",
            "\tTraining batch 37 Loss: 0.303644\n",
            "\tTraining batch 38 Loss: 0.285364\n",
            "\tTraining batch 39 Loss: 0.417435\n",
            "\tTraining batch 40 Loss: 0.173075\n",
            "\tTraining batch 41 Loss: 0.352919\n",
            "\tTraining batch 42 Loss: 0.290751\n",
            "\tTraining batch 43 Loss: 0.258759\n",
            "\tTraining batch 44 Loss: 0.269432\n",
            "\tTraining batch 45 Loss: 0.381317\n",
            "\tTraining batch 46 Loss: 0.582611\n",
            "\tTraining batch 47 Loss: 0.391950\n",
            "\tTraining batch 48 Loss: 0.377459\n",
            "\tTraining batch 49 Loss: 0.344890\n",
            "\tTraining batch 50 Loss: 0.442411\n",
            "\tTraining batch 51 Loss: 0.396738\n",
            "\tTraining batch 52 Loss: 0.453491\n",
            "\tTraining batch 53 Loss: 0.576095\n",
            "\tTraining batch 54 Loss: 0.456060\n",
            "\tTraining batch 55 Loss: 0.448131\n",
            "\tTraining batch 56 Loss: 0.305854\n",
            "\tTraining batch 57 Loss: 0.232319\n",
            "\tTraining batch 58 Loss: 0.294883\n",
            "\tTraining batch 59 Loss: 0.586526\n",
            "\tTraining batch 60 Loss: 0.385219\n",
            "\tTraining batch 61 Loss: 0.655751\n",
            "\tTraining batch 62 Loss: 0.426254\n",
            "\tTraining batch 63 Loss: 0.272941\n",
            "\tTraining batch 64 Loss: 0.430789\n",
            "\tTraining batch 65 Loss: 0.362239\n",
            "\tTraining batch 66 Loss: 0.500531\n",
            "\tTraining batch 67 Loss: 0.197410\n",
            "\tTraining batch 68 Loss: 0.307647\n",
            "\tTraining batch 69 Loss: 0.257451\n",
            "\tTraining batch 70 Loss: 0.460985\n",
            "\tTraining batch 71 Loss: 0.347902\n",
            "\tTraining batch 72 Loss: 0.559644\n",
            "\tTraining batch 73 Loss: 0.373425\n",
            "\tTraining batch 74 Loss: 0.486046\n",
            "\tTraining batch 75 Loss: 0.419745\n",
            "\tTraining batch 76 Loss: 0.320040\n",
            "\tTraining batch 77 Loss: 0.348451\n",
            "\tTraining batch 78 Loss: 0.286250\n",
            "\tTraining batch 79 Loss: 0.493001\n",
            "\tTraining batch 80 Loss: 0.351412\n",
            "\tTraining batch 81 Loss: 0.553344\n",
            "\tTraining batch 82 Loss: 0.167040\n",
            "\tTraining batch 83 Loss: 0.268294\n",
            "\tTraining batch 84 Loss: 0.300869\n",
            "\tTraining batch 85 Loss: 0.134896\n",
            "\tTraining batch 86 Loss: 0.266482\n",
            "\tTraining batch 87 Loss: 0.606380\n",
            "\tTraining batch 88 Loss: 0.285685\n",
            "\tTraining batch 89 Loss: 0.306120\n",
            "\tTraining batch 90 Loss: 0.334921\n",
            "\tTraining batch 91 Loss: 0.398311\n",
            "\tTraining batch 92 Loss: 0.287860\n",
            "\tTraining batch 93 Loss: 0.575245\n",
            "\tTraining batch 94 Loss: 0.229981\n",
            "\tTraining batch 95 Loss: 0.223786\n",
            "\tTraining batch 96 Loss: 0.490745\n",
            "\tTraining batch 97 Loss: 0.509829\n",
            "Training set: Average loss: 0.389690\n",
            "Validation set: Average loss: 0.317645, Accuracy: 1804/2070 (87%)\n",
            "\n",
            "Epoch: 10\n",
            "\tTraining batch 1 Loss: 0.215580\n",
            "\tTraining batch 2 Loss: 0.400570\n",
            "\tTraining batch 3 Loss: 0.431857\n",
            "\tTraining batch 4 Loss: 0.457938\n",
            "\tTraining batch 5 Loss: 0.393525\n",
            "\tTraining batch 6 Loss: 0.371742\n",
            "\tTraining batch 7 Loss: 0.244514\n",
            "\tTraining batch 8 Loss: 0.454677\n",
            "\tTraining batch 9 Loss: 0.197725\n",
            "\tTraining batch 10 Loss: 0.324622\n",
            "\tTraining batch 11 Loss: 0.438288\n",
            "\tTraining batch 12 Loss: 0.288834\n",
            "\tTraining batch 13 Loss: 0.297090\n",
            "\tTraining batch 14 Loss: 0.244109\n",
            "\tTraining batch 15 Loss: 0.429189\n",
            "\tTraining batch 16 Loss: 0.376998\n",
            "\tTraining batch 17 Loss: 0.506088\n",
            "\tTraining batch 18 Loss: 0.253847\n",
            "\tTraining batch 19 Loss: 0.438082\n",
            "\tTraining batch 20 Loss: 0.878997\n",
            "\tTraining batch 21 Loss: 0.137446\n",
            "\tTraining batch 22 Loss: 0.544574\n",
            "\tTraining batch 23 Loss: 0.506861\n",
            "\tTraining batch 24 Loss: 0.316770\n",
            "\tTraining batch 25 Loss: 0.412908\n",
            "\tTraining batch 26 Loss: 0.513011\n",
            "\tTraining batch 27 Loss: 0.392239\n",
            "\tTraining batch 28 Loss: 0.492679\n",
            "\tTraining batch 29 Loss: 0.127278\n",
            "\tTraining batch 30 Loss: 0.724068\n",
            "\tTraining batch 31 Loss: 0.575240\n",
            "\tTraining batch 32 Loss: 0.350005\n",
            "\tTraining batch 33 Loss: 0.330589\n",
            "\tTraining batch 34 Loss: 0.306604\n",
            "\tTraining batch 35 Loss: 0.307523\n",
            "\tTraining batch 36 Loss: 0.918360\n",
            "\tTraining batch 37 Loss: 0.270447\n",
            "\tTraining batch 38 Loss: 0.090224\n",
            "\tTraining batch 39 Loss: 0.477621\n",
            "\tTraining batch 40 Loss: 0.277689\n",
            "\tTraining batch 41 Loss: 0.325278\n",
            "\tTraining batch 42 Loss: 0.275837\n",
            "\tTraining batch 43 Loss: 0.253396\n",
            "\tTraining batch 44 Loss: 0.292126\n",
            "\tTraining batch 45 Loss: 0.354066\n",
            "\tTraining batch 46 Loss: 0.300094\n",
            "\tTraining batch 47 Loss: 0.556521\n",
            "\tTraining batch 48 Loss: 0.332499\n",
            "\tTraining batch 49 Loss: 0.488544\n",
            "\tTraining batch 50 Loss: 0.533675\n",
            "\tTraining batch 51 Loss: 0.399599\n",
            "\tTraining batch 52 Loss: 0.426033\n",
            "\tTraining batch 53 Loss: 0.563066\n",
            "\tTraining batch 54 Loss: 0.299225\n",
            "\tTraining batch 55 Loss: 0.596873\n",
            "\tTraining batch 56 Loss: 0.575532\n",
            "\tTraining batch 57 Loss: 0.283631\n",
            "\tTraining batch 58 Loss: 0.285560\n",
            "\tTraining batch 59 Loss: 0.483531\n",
            "\tTraining batch 60 Loss: 0.423469\n",
            "\tTraining batch 61 Loss: 0.479524\n",
            "\tTraining batch 62 Loss: 0.242024\n",
            "\tTraining batch 63 Loss: 0.340023\n",
            "\tTraining batch 64 Loss: 0.303990\n",
            "\tTraining batch 65 Loss: 0.273993\n",
            "\tTraining batch 66 Loss: 0.368834\n",
            "\tTraining batch 67 Loss: 0.582135\n",
            "\tTraining batch 68 Loss: 0.323422\n",
            "\tTraining batch 69 Loss: 0.252664\n",
            "\tTraining batch 70 Loss: 0.242119\n",
            "\tTraining batch 71 Loss: 0.367343\n",
            "\tTraining batch 72 Loss: 0.460398\n",
            "\tTraining batch 73 Loss: 0.670769\n",
            "\tTraining batch 74 Loss: 0.251815\n",
            "\tTraining batch 75 Loss: 0.220071\n",
            "\tTraining batch 76 Loss: 0.251227\n",
            "\tTraining batch 77 Loss: 0.320876\n",
            "\tTraining batch 78 Loss: 0.402557\n",
            "\tTraining batch 79 Loss: 0.371436\n",
            "\tTraining batch 80 Loss: 0.368226\n",
            "\tTraining batch 81 Loss: 0.486412\n",
            "\tTraining batch 82 Loss: 0.267487\n",
            "\tTraining batch 83 Loss: 0.209389\n",
            "\tTraining batch 84 Loss: 0.282732\n",
            "\tTraining batch 85 Loss: 0.451458\n",
            "\tTraining batch 86 Loss: 0.276844\n",
            "\tTraining batch 87 Loss: 0.272532\n",
            "\tTraining batch 88 Loss: 0.542886\n",
            "\tTraining batch 89 Loss: 0.355977\n",
            "\tTraining batch 90 Loss: 0.370555\n",
            "\tTraining batch 91 Loss: 0.375482\n",
            "\tTraining batch 92 Loss: 0.196394\n",
            "\tTraining batch 93 Loss: 0.518548\n",
            "\tTraining batch 94 Loss: 0.567814\n",
            "\tTraining batch 95 Loss: 0.235751\n",
            "\tTraining batch 96 Loss: 0.338469\n",
            "\tTraining batch 97 Loss: 0.328206\n",
            "Training set: Average loss: 0.380777\n",
            "Validation set: Average loss: 0.331784, Accuracy: 1798/2070 (87%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Labels and Predictions\n",
        "truelabels = []\n",
        "predictions = []\n",
        "model.eval()\n",
        "print(\"Getting predictions from test set...\")\n",
        "for data, target in test_loader:\n",
        "    for label in target.data.numpy():\n",
        "        truelabels.append(label)\n",
        "    for prediction in model(data).data.numpy().argmax(1):\n",
        "        predictions.append(prediction) \n",
        "\n",
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(truelabels, predictions)\n",
        "tick_marks = np.arange(len(classes))\n",
        "\n",
        "df_cm = pd.DataFrame(cm, index = classes, columns = classes)\n",
        "plt.figure(figsize = (7,7))\n",
        "sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\n",
        "plt.xlabel(\"Predicted Shape\", fontsize = 20)\n",
        "plt.ylabel(\"True Shape\", fontsize = 20)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "id": "iDXB4rP9Xkzf",
        "outputId": "3aa4bbc5-2327-4b86-99d1-3b122cc0a764"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting predictions from test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 504x504 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the training and validation loss\n",
        "plt.plot(training_loss, label='Training loss')\n",
        "plt.plot(validation_loss, label='Validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "W0J8OZ0JXonq",
        "outputId": "68470d0d-e5a8-4666-f386-fb50b865113a"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Allconvo(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Allconvo, self).__init__()\n",
        "        # 3 input image channel, 16 output channels, 3x3 square convolution kernel\n",
        "        self.conv1 = nn.Conv2d(3,16,kernel_size=3,stride=2,padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32,kernel_size=3,stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64,kernel_size=3,stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(64, 64,kernel_size=3,stride=2, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout2d(0.4)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(64)\n",
        "        self.fc1 = nn.Linear(64*5*5,512 )\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 4)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.batchnorm1(F.relu(self.conv1(x)))\n",
        "        x = self.batchnorm2(F.relu(self.conv2(x)))\n",
        "        x = self.dropout(self.batchnorm2(self.pool(x)))\n",
        "        x = self.batchnorm3(self.pool(F.relu(self.conv3(x))))\n",
        "        x = self.dropout(self.conv4(x))\n",
        "        x = x.view(-1, 64*5*5) # Flatten layer\n",
        "        x = self.dropout(self.fc1(x))\n",
        "        x = self.dropout(self.fc2(x))\n",
        "        x = F.log_softmax(self.fc3(x),dim = 1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "nJ_USVqw1mxA"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Allconvo() # On CPU\n",
        "#model = Net().to(device)  # On GPU\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBqaPvCLU3zU",
        "outputId": "7a6f2b6a-15cc-46bf-8f92-e214ecfb1e60"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allconvo(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout): Dropout2d(p=0.4, inplace=False)\n",
            "  (batchnorm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=1600, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (fc3): Linear(in_features=256, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    print(\"Epoch:\", epoch)\n",
        "    # Process the images in batches\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Use the CPU or GPU as appropriate\n",
        "        # Recall that GPU is optimized for the operations we are dealing with\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # Reset the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Push the data forward through the model layers\n",
        "        output = model(data)\n",
        "        \n",
        "        # Get the loss\n",
        "        loss = loss_criteria(output, target)\n",
        "\n",
        "        # Keep a running total\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Print metrics so we see some progress\n",
        "        print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
        "            \n",
        "    # return average loss for the epoch\n",
        "    avg_loss = train_loss / (batch_idx+1)\n",
        "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "7NzUqv5BU31r"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader):\n",
        "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        batch_count = 0\n",
        "        for data, target in test_loader:\n",
        "            batch_count += 1\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            # Get the predicted classes for this batch\n",
        "            output = model(data)\n",
        "            \n",
        "            # Calculate the loss for this batch\n",
        "            test_loss += loss_criteria(output, target).item()\n",
        "            \n",
        "            # Calculate the accuracy for this batch\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            correct += torch.sum(target==predicted).item()\n",
        "\n",
        "    # Calculate the average loss and total accuracy for this epoch\n",
        "    avg_loss = test_loss / batch_count\n",
        "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        avg_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    # return average loss for the epoch\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "un_YW4ZGU35O"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use an \"Adam\" optimizer to adjust weights\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Specify the loss criteria\n",
        "loss_criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "# Track metrics in these arrays\n",
        "epoch_nums = []\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "\n",
        "# Train over 10 epochs (We restrict to 10 for time issues)\n",
        "epochs = 10\n",
        "print('Training on', device)\n",
        "for epoch in range(1, epochs + 1):\n",
        "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
        "        test_loss = test(model, device, test_loader)\n",
        "        epoch_nums.append(epoch)\n",
        "        training_loss.append(train_loss)\n",
        "        validation_loss.append(test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK1Tg6gxU38u",
        "outputId": "23aeb2a5-afd5-4731-bfb5-5d988378c3b3"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cpu\n",
            "Epoch: 1\n",
            "\tTraining batch 1 Loss: 0.246028\n",
            "\tTraining batch 2 Loss: 0.317498\n",
            "\tTraining batch 3 Loss: 0.398813\n",
            "\tTraining batch 4 Loss: 0.292781\n",
            "\tTraining batch 5 Loss: 0.275676\n",
            "\tTraining batch 6 Loss: 0.234841\n",
            "\tTraining batch 7 Loss: 0.316944\n",
            "\tTraining batch 8 Loss: 0.470033\n",
            "\tTraining batch 9 Loss: 0.185267\n",
            "\tTraining batch 10 Loss: 0.427671\n",
            "\tTraining batch 11 Loss: 0.203388\n",
            "\tTraining batch 12 Loss: 0.254126\n",
            "\tTraining batch 13 Loss: 0.423377\n",
            "\tTraining batch 14 Loss: 0.366853\n",
            "\tTraining batch 15 Loss: 0.383312\n",
            "\tTraining batch 16 Loss: 0.329504\n",
            "\tTraining batch 17 Loss: 0.418749\n",
            "\tTraining batch 18 Loss: 0.425040\n",
            "\tTraining batch 19 Loss: 0.274665\n",
            "\tTraining batch 20 Loss: 0.457213\n",
            "\tTraining batch 21 Loss: 0.270890\n",
            "\tTraining batch 22 Loss: 0.505458\n",
            "\tTraining batch 23 Loss: 0.326593\n",
            "\tTraining batch 24 Loss: 0.513826\n",
            "\tTraining batch 25 Loss: 0.312812\n",
            "\tTraining batch 26 Loss: 0.499818\n",
            "\tTraining batch 27 Loss: 0.454931\n",
            "\tTraining batch 28 Loss: 0.292242\n",
            "\tTraining batch 29 Loss: 0.310816\n",
            "\tTraining batch 30 Loss: 0.292031\n",
            "\tTraining batch 31 Loss: 0.369074\n",
            "\tTraining batch 32 Loss: 0.269256\n",
            "\tTraining batch 33 Loss: 0.363336\n",
            "\tTraining batch 34 Loss: 0.413357\n",
            "\tTraining batch 35 Loss: 0.252314\n",
            "\tTraining batch 36 Loss: 0.533902\n",
            "\tTraining batch 37 Loss: 0.341293\n",
            "\tTraining batch 38 Loss: 0.170431\n",
            "\tTraining batch 39 Loss: 0.331121\n",
            "\tTraining batch 40 Loss: 0.248838\n",
            "\tTraining batch 41 Loss: 0.351872\n",
            "\tTraining batch 42 Loss: 0.156349\n",
            "\tTraining batch 43 Loss: 0.322284\n",
            "\tTraining batch 44 Loss: 0.238595\n",
            "\tTraining batch 45 Loss: 0.421046\n",
            "\tTraining batch 46 Loss: 0.295715\n",
            "\tTraining batch 47 Loss: 0.630623\n",
            "\tTraining batch 48 Loss: 0.475330\n",
            "\tTraining batch 49 Loss: 0.288644\n",
            "\tTraining batch 50 Loss: 0.445510\n",
            "\tTraining batch 51 Loss: 0.268022\n",
            "\tTraining batch 52 Loss: 0.504233\n",
            "\tTraining batch 53 Loss: 0.377221\n",
            "\tTraining batch 54 Loss: 0.312680\n",
            "\tTraining batch 55 Loss: 0.478187\n",
            "\tTraining batch 56 Loss: 0.345784\n",
            "\tTraining batch 57 Loss: 0.311628\n",
            "\tTraining batch 58 Loss: 0.291599\n",
            "\tTraining batch 59 Loss: 0.517531\n",
            "\tTraining batch 60 Loss: 0.328177\n",
            "\tTraining batch 61 Loss: 0.376857\n",
            "\tTraining batch 62 Loss: 0.295994\n",
            "\tTraining batch 63 Loss: 0.450273\n",
            "\tTraining batch 64 Loss: 0.300806\n",
            "\tTraining batch 65 Loss: 0.204735\n",
            "\tTraining batch 66 Loss: 0.297049\n",
            "\tTraining batch 67 Loss: 0.228612\n",
            "\tTraining batch 68 Loss: 0.220128\n",
            "\tTraining batch 69 Loss: 0.264463\n",
            "\tTraining batch 70 Loss: 0.352403\n",
            "\tTraining batch 71 Loss: 0.433350\n",
            "\tTraining batch 72 Loss: 0.346686\n",
            "\tTraining batch 73 Loss: 0.276325\n",
            "\tTraining batch 74 Loss: 0.238689\n",
            "\tTraining batch 75 Loss: 0.274597\n",
            "\tTraining batch 76 Loss: 0.314572\n",
            "\tTraining batch 77 Loss: 0.458536\n",
            "\tTraining batch 78 Loss: 0.336908\n",
            "\tTraining batch 79 Loss: 0.461164\n",
            "\tTraining batch 80 Loss: 0.311899\n",
            "\tTraining batch 81 Loss: 0.499613\n",
            "\tTraining batch 82 Loss: 0.166592\n",
            "\tTraining batch 83 Loss: 0.251701\n",
            "\tTraining batch 84 Loss: 0.378380\n",
            "\tTraining batch 85 Loss: 0.220128\n",
            "\tTraining batch 86 Loss: 0.363051\n",
            "\tTraining batch 87 Loss: 0.328761\n",
            "\tTraining batch 88 Loss: 0.128116\n",
            "\tTraining batch 89 Loss: 0.379769\n",
            "\tTraining batch 90 Loss: 0.378455\n",
            "\tTraining batch 91 Loss: 0.285684\n",
            "\tTraining batch 92 Loss: 0.214290\n",
            "\tTraining batch 93 Loss: 0.354260\n",
            "\tTraining batch 94 Loss: 0.228438\n",
            "\tTraining batch 95 Loss: 0.088984\n",
            "\tTraining batch 96 Loss: 0.247925\n",
            "\tTraining batch 97 Loss: 0.376306\n",
            "Training set: Average loss: 0.334739\n",
            "Validation set: Average loss: 0.297928, Accuracy: 1823/2070 (88%)\n",
            "\n",
            "Epoch: 2\n",
            "\tTraining batch 1 Loss: 0.252958\n",
            "\tTraining batch 2 Loss: 0.208547\n",
            "\tTraining batch 3 Loss: 0.358345\n",
            "\tTraining batch 4 Loss: 0.403906\n",
            "\tTraining batch 5 Loss: 0.191193\n",
            "\tTraining batch 6 Loss: 0.356572\n",
            "\tTraining batch 7 Loss: 0.178011\n",
            "\tTraining batch 8 Loss: 0.528396\n",
            "\tTraining batch 9 Loss: 0.222130\n",
            "\tTraining batch 10 Loss: 0.294605\n",
            "\tTraining batch 11 Loss: 0.229653\n",
            "\tTraining batch 12 Loss: 0.273077\n",
            "\tTraining batch 13 Loss: 0.295867\n",
            "\tTraining batch 14 Loss: 0.435851\n",
            "\tTraining batch 15 Loss: 0.388022\n",
            "\tTraining batch 16 Loss: 0.224531\n",
            "\tTraining batch 17 Loss: 0.256661\n",
            "\tTraining batch 18 Loss: 0.218509\n",
            "\tTraining batch 19 Loss: 0.227463\n",
            "\tTraining batch 20 Loss: 0.383031\n",
            "\tTraining batch 21 Loss: 0.232014\n",
            "\tTraining batch 22 Loss: 0.294878\n",
            "\tTraining batch 23 Loss: 0.375887\n",
            "\tTraining batch 24 Loss: 0.381900\n",
            "\tTraining batch 25 Loss: 0.135909\n",
            "\tTraining batch 26 Loss: 0.289639\n",
            "\tTraining batch 27 Loss: 0.354602\n",
            "\tTraining batch 28 Loss: 0.359129\n",
            "\tTraining batch 29 Loss: 0.190650\n",
            "\tTraining batch 30 Loss: 0.374184\n",
            "\tTraining batch 31 Loss: 0.234277\n",
            "\tTraining batch 32 Loss: 0.309983\n",
            "\tTraining batch 33 Loss: 0.335105\n",
            "\tTraining batch 34 Loss: 0.334149\n",
            "\tTraining batch 35 Loss: 0.255004\n",
            "\tTraining batch 36 Loss: 0.471469\n",
            "\tTraining batch 37 Loss: 0.304242\n",
            "\tTraining batch 38 Loss: 0.180600\n",
            "\tTraining batch 39 Loss: 0.375192\n",
            "\tTraining batch 40 Loss: 0.400916\n",
            "\tTraining batch 41 Loss: 0.186423\n",
            "\tTraining batch 42 Loss: 0.220936\n",
            "\tTraining batch 43 Loss: 0.221718\n",
            "\tTraining batch 44 Loss: 0.227126\n",
            "\tTraining batch 45 Loss: 0.256857\n",
            "\tTraining batch 46 Loss: 0.294210\n",
            "\tTraining batch 47 Loss: 0.506336\n",
            "\tTraining batch 48 Loss: 0.245095\n",
            "\tTraining batch 49 Loss: 0.280193\n",
            "\tTraining batch 50 Loss: 0.255314\n",
            "\tTraining batch 51 Loss: 0.435237\n",
            "\tTraining batch 52 Loss: 0.514397\n",
            "\tTraining batch 53 Loss: 0.378347\n",
            "\tTraining batch 54 Loss: 0.365626\n",
            "\tTraining batch 55 Loss: 0.475073\n",
            "\tTraining batch 56 Loss: 0.397151\n",
            "\tTraining batch 57 Loss: 0.339023\n",
            "\tTraining batch 58 Loss: 0.136954\n",
            "\tTraining batch 59 Loss: 0.367412\n",
            "\tTraining batch 60 Loss: 0.469176\n",
            "\tTraining batch 61 Loss: 0.454706\n",
            "\tTraining batch 62 Loss: 0.346260\n",
            "\tTraining batch 63 Loss: 0.316581\n",
            "\tTraining batch 64 Loss: 0.278504\n",
            "\tTraining batch 65 Loss: 0.374047\n",
            "\tTraining batch 66 Loss: 0.336952\n",
            "\tTraining batch 67 Loss: 0.213537\n",
            "\tTraining batch 68 Loss: 0.309700\n",
            "\tTraining batch 69 Loss: 0.354402\n",
            "\tTraining batch 70 Loss: 0.285821\n",
            "\tTraining batch 71 Loss: 0.341855\n",
            "\tTraining batch 72 Loss: 0.400681\n",
            "\tTraining batch 73 Loss: 0.390136\n",
            "\tTraining batch 74 Loss: 0.166544\n",
            "\tTraining batch 75 Loss: 0.257087\n",
            "\tTraining batch 76 Loss: 0.291440\n",
            "\tTraining batch 77 Loss: 0.322583\n",
            "\tTraining batch 78 Loss: 0.399008\n",
            "\tTraining batch 79 Loss: 0.266937\n",
            "\tTraining batch 80 Loss: 0.187449\n",
            "\tTraining batch 81 Loss: 0.466416\n",
            "\tTraining batch 82 Loss: 0.205740\n",
            "\tTraining batch 83 Loss: 0.197873\n",
            "\tTraining batch 84 Loss: 0.291404\n",
            "\tTraining batch 85 Loss: 0.114777\n",
            "\tTraining batch 86 Loss: 0.197334\n",
            "\tTraining batch 87 Loss: 0.348523\n",
            "\tTraining batch 88 Loss: 0.221589\n",
            "\tTraining batch 89 Loss: 0.259031\n",
            "\tTraining batch 90 Loss: 0.213246\n",
            "\tTraining batch 91 Loss: 0.257629\n",
            "\tTraining batch 92 Loss: 0.139682\n",
            "\tTraining batch 93 Loss: 0.409506\n",
            "\tTraining batch 94 Loss: 0.285895\n",
            "\tTraining batch 95 Loss: 0.252298\n",
            "\tTraining batch 96 Loss: 0.437125\n",
            "\tTraining batch 97 Loss: 0.164907\n",
            "Training set: Average loss: 0.303576\n",
            "Validation set: Average loss: 0.272231, Accuracy: 1825/2070 (88%)\n",
            "\n",
            "Epoch: 3\n",
            "\tTraining batch 1 Loss: 0.259825\n",
            "\tTraining batch 2 Loss: 0.403175\n",
            "\tTraining batch 3 Loss: 0.317198\n",
            "\tTraining batch 4 Loss: 0.186375\n",
            "\tTraining batch 5 Loss: 0.170829\n",
            "\tTraining batch 6 Loss: 0.259021\n",
            "\tTraining batch 7 Loss: 0.319926\n",
            "\tTraining batch 8 Loss: 0.463430\n",
            "\tTraining batch 9 Loss: 0.242842\n",
            "\tTraining batch 10 Loss: 0.276385\n",
            "\tTraining batch 11 Loss: 0.316066\n",
            "\tTraining batch 12 Loss: 0.300871\n",
            "\tTraining batch 13 Loss: 0.217067\n",
            "\tTraining batch 14 Loss: 0.246916\n",
            "\tTraining batch 15 Loss: 0.255094\n",
            "\tTraining batch 16 Loss: 0.225862\n",
            "\tTraining batch 17 Loss: 0.412731\n",
            "\tTraining batch 18 Loss: 0.310317\n",
            "\tTraining batch 19 Loss: 0.263719\n",
            "\tTraining batch 20 Loss: 0.356257\n",
            "\tTraining batch 21 Loss: 0.130156\n",
            "\tTraining batch 22 Loss: 0.320504\n",
            "\tTraining batch 23 Loss: 0.482057\n",
            "\tTraining batch 24 Loss: 0.368922\n",
            "\tTraining batch 25 Loss: 0.285649\n",
            "\tTraining batch 26 Loss: 0.207747\n",
            "\tTraining batch 27 Loss: 0.359529\n",
            "\tTraining batch 28 Loss: 0.289969\n",
            "\tTraining batch 29 Loss: 0.277395\n",
            "\tTraining batch 30 Loss: 0.460380\n",
            "\tTraining batch 31 Loss: 0.473731\n",
            "\tTraining batch 32 Loss: 0.269012\n",
            "\tTraining batch 33 Loss: 0.260116\n",
            "\tTraining batch 34 Loss: 0.322530\n",
            "\tTraining batch 35 Loss: 0.170968\n",
            "\tTraining batch 36 Loss: 0.322723\n",
            "\tTraining batch 37 Loss: 0.093260\n",
            "\tTraining batch 38 Loss: 0.144265\n",
            "\tTraining batch 39 Loss: 0.320392\n",
            "\tTraining batch 40 Loss: 0.331375\n",
            "\tTraining batch 41 Loss: 0.335508\n",
            "\tTraining batch 42 Loss: 0.239232\n",
            "\tTraining batch 43 Loss: 0.370402\n",
            "\tTraining batch 44 Loss: 0.320336\n",
            "\tTraining batch 45 Loss: 0.320132\n",
            "\tTraining batch 46 Loss: 0.409272\n",
            "\tTraining batch 47 Loss: 0.504359\n",
            "\tTraining batch 48 Loss: 0.315749\n",
            "\tTraining batch 49 Loss: 0.321887\n",
            "\tTraining batch 50 Loss: 0.268440\n",
            "\tTraining batch 51 Loss: 0.355011\n",
            "\tTraining batch 52 Loss: 0.381093\n",
            "\tTraining batch 53 Loss: 0.559863\n",
            "\tTraining batch 54 Loss: 0.193992\n",
            "\tTraining batch 55 Loss: 0.454819\n",
            "\tTraining batch 56 Loss: 0.249072\n",
            "\tTraining batch 57 Loss: 0.157973\n",
            "\tTraining batch 58 Loss: 0.107792\n",
            "\tTraining batch 59 Loss: 0.506586\n",
            "\tTraining batch 60 Loss: 0.187859\n",
            "\tTraining batch 61 Loss: 0.334069\n",
            "\tTraining batch 62 Loss: 0.076537\n",
            "\tTraining batch 63 Loss: 0.375187\n",
            "\tTraining batch 64 Loss: 0.242482\n",
            "\tTraining batch 65 Loss: 0.226723\n",
            "\tTraining batch 66 Loss: 0.309941\n",
            "\tTraining batch 67 Loss: 0.238714\n",
            "\tTraining batch 68 Loss: 0.354667\n",
            "\tTraining batch 69 Loss: 0.269257\n",
            "\tTraining batch 70 Loss: 0.299440\n",
            "\tTraining batch 71 Loss: 0.270268\n",
            "\tTraining batch 72 Loss: 0.246835\n",
            "\tTraining batch 73 Loss: 0.345477\n",
            "\tTraining batch 74 Loss: 0.176612\n",
            "\tTraining batch 75 Loss: 0.217682\n",
            "\tTraining batch 76 Loss: 0.236417\n",
            "\tTraining batch 77 Loss: 0.356945\n",
            "\tTraining batch 78 Loss: 0.337757\n",
            "\tTraining batch 79 Loss: 0.320776\n",
            "\tTraining batch 80 Loss: 0.241747\n",
            "\tTraining batch 81 Loss: 0.406304\n",
            "\tTraining batch 82 Loss: 0.100425\n",
            "\tTraining batch 83 Loss: 0.263027\n",
            "\tTraining batch 84 Loss: 0.369761\n",
            "\tTraining batch 85 Loss: 0.170022\n",
            "\tTraining batch 86 Loss: 0.198857\n",
            "\tTraining batch 87 Loss: 0.268631\n",
            "\tTraining batch 88 Loss: 0.205506\n",
            "\tTraining batch 89 Loss: 0.277802\n",
            "\tTraining batch 90 Loss: 0.393361\n",
            "\tTraining batch 91 Loss: 0.259779\n",
            "\tTraining batch 92 Loss: 0.100671\n",
            "\tTraining batch 93 Loss: 0.344600\n",
            "\tTraining batch 94 Loss: 0.244939\n",
            "\tTraining batch 95 Loss: 0.211825\n",
            "\tTraining batch 96 Loss: 0.315484\n",
            "\tTraining batch 97 Loss: 0.270364\n",
            "Training set: Average loss: 0.290029\n",
            "Validation set: Average loss: 0.278700, Accuracy: 1833/2070 (89%)\n",
            "\n",
            "Epoch: 4\n",
            "\tTraining batch 1 Loss: 0.275526\n",
            "\tTraining batch 2 Loss: 0.244411\n",
            "\tTraining batch 3 Loss: 0.384477\n",
            "\tTraining batch 4 Loss: 0.433952\n",
            "\tTraining batch 5 Loss: 0.137802\n",
            "\tTraining batch 6 Loss: 0.223742\n",
            "\tTraining batch 7 Loss: 0.241263\n",
            "\tTraining batch 8 Loss: 0.464407\n",
            "\tTraining batch 9 Loss: 0.293807\n",
            "\tTraining batch 10 Loss: 0.273338\n",
            "\tTraining batch 11 Loss: 0.361495\n",
            "\tTraining batch 12 Loss: 0.224910\n",
            "\tTraining batch 13 Loss: 0.329740\n",
            "\tTraining batch 14 Loss: 0.208529\n",
            "\tTraining batch 15 Loss: 0.360017\n",
            "\tTraining batch 16 Loss: 0.233903\n",
            "\tTraining batch 17 Loss: 0.246446\n",
            "\tTraining batch 18 Loss: 0.175055\n",
            "\tTraining batch 19 Loss: 0.239756\n",
            "\tTraining batch 20 Loss: 0.282892\n",
            "\tTraining batch 21 Loss: 0.259109\n",
            "\tTraining batch 22 Loss: 0.292633\n",
            "\tTraining batch 23 Loss: 0.253520\n",
            "\tTraining batch 24 Loss: 0.269870\n",
            "\tTraining batch 25 Loss: 0.261928\n",
            "\tTraining batch 26 Loss: 0.394346\n",
            "\tTraining batch 27 Loss: 0.221456\n",
            "\tTraining batch 28 Loss: 0.345880\n",
            "\tTraining batch 29 Loss: 0.265530\n",
            "\tTraining batch 30 Loss: 0.269011\n",
            "\tTraining batch 31 Loss: 0.286369\n",
            "\tTraining batch 32 Loss: 0.238281\n",
            "\tTraining batch 33 Loss: 0.341813\n",
            "\tTraining batch 34 Loss: 0.280293\n",
            "\tTraining batch 35 Loss: 0.267195\n",
            "\tTraining batch 36 Loss: 0.663462\n",
            "\tTraining batch 37 Loss: 0.268929\n",
            "\tTraining batch 38 Loss: 0.209827\n",
            "\tTraining batch 39 Loss: 0.329569\n",
            "\tTraining batch 40 Loss: 0.349261\n",
            "\tTraining batch 41 Loss: 0.173854\n",
            "\tTraining batch 42 Loss: 0.191901\n",
            "\tTraining batch 43 Loss: 0.270790\n",
            "\tTraining batch 44 Loss: 0.145827\n",
            "\tTraining batch 45 Loss: 0.271093\n",
            "\tTraining batch 46 Loss: 0.239929\n",
            "\tTraining batch 47 Loss: 0.455322\n",
            "\tTraining batch 48 Loss: 0.246329\n",
            "\tTraining batch 49 Loss: 0.284597\n",
            "\tTraining batch 50 Loss: 0.285348\n",
            "\tTraining batch 51 Loss: 0.408584\n",
            "\tTraining batch 52 Loss: 0.540720\n",
            "\tTraining batch 53 Loss: 0.331615\n",
            "\tTraining batch 54 Loss: 0.410286\n",
            "\tTraining batch 55 Loss: 0.272565\n",
            "\tTraining batch 56 Loss: 0.380296\n",
            "\tTraining batch 57 Loss: 0.160104\n",
            "\tTraining batch 58 Loss: 0.153831\n",
            "\tTraining batch 59 Loss: 0.433419\n",
            "\tTraining batch 60 Loss: 0.320706\n",
            "\tTraining batch 61 Loss: 0.270809\n",
            "\tTraining batch 62 Loss: 0.388633\n",
            "\tTraining batch 63 Loss: 0.255587\n",
            "\tTraining batch 64 Loss: 0.263678\n",
            "\tTraining batch 65 Loss: 0.307614\n",
            "\tTraining batch 66 Loss: 0.204285\n",
            "\tTraining batch 67 Loss: 0.196907\n",
            "\tTraining batch 68 Loss: 0.356431\n",
            "\tTraining batch 69 Loss: 0.215993\n",
            "\tTraining batch 70 Loss: 0.264569\n",
            "\tTraining batch 71 Loss: 0.367904\n",
            "\tTraining batch 72 Loss: 0.464400\n",
            "\tTraining batch 73 Loss: 0.239356\n",
            "\tTraining batch 74 Loss: 0.297873\n",
            "\tTraining batch 75 Loss: 0.267075\n",
            "\tTraining batch 76 Loss: 0.272463\n",
            "\tTraining batch 77 Loss: 0.346758\n",
            "\tTraining batch 78 Loss: 0.312510\n",
            "\tTraining batch 79 Loss: 0.199932\n",
            "\tTraining batch 80 Loss: 0.277575\n",
            "\tTraining batch 81 Loss: 0.331966\n",
            "\tTraining batch 82 Loss: 0.171030\n",
            "\tTraining batch 83 Loss: 0.234774\n",
            "\tTraining batch 84 Loss: 0.212541\n",
            "\tTraining batch 85 Loss: 0.158179\n",
            "\tTraining batch 86 Loss: 0.218680\n",
            "\tTraining batch 87 Loss: 0.243204\n",
            "\tTraining batch 88 Loss: 0.235988\n",
            "\tTraining batch 89 Loss: 0.306570\n",
            "\tTraining batch 90 Loss: 0.171790\n",
            "\tTraining batch 91 Loss: 0.305808\n",
            "\tTraining batch 92 Loss: 0.109286\n",
            "\tTraining batch 93 Loss: 0.249091\n",
            "\tTraining batch 94 Loss: 0.151481\n",
            "\tTraining batch 95 Loss: 0.163795\n",
            "\tTraining batch 96 Loss: 0.228516\n",
            "\tTraining batch 97 Loss: 0.232575\n",
            "Training set: Average loss: 0.280170\n",
            "Validation set: Average loss: 0.274237, Accuracy: 1833/2070 (89%)\n",
            "\n",
            "Epoch: 5\n",
            "\tTraining batch 1 Loss: 0.239942\n",
            "\tTraining batch 2 Loss: 0.251739\n",
            "\tTraining batch 3 Loss: 0.208719\n",
            "\tTraining batch 4 Loss: 0.235471\n",
            "\tTraining batch 5 Loss: 0.330517\n",
            "\tTraining batch 6 Loss: 0.276930\n",
            "\tTraining batch 7 Loss: 0.289462\n",
            "\tTraining batch 8 Loss: 0.448788\n",
            "\tTraining batch 9 Loss: 0.264238\n",
            "\tTraining batch 10 Loss: 0.384888\n",
            "\tTraining batch 11 Loss: 0.216689\n",
            "\tTraining batch 12 Loss: 0.299743\n",
            "\tTraining batch 13 Loss: 0.217142\n",
            "\tTraining batch 14 Loss: 0.388665\n",
            "\tTraining batch 15 Loss: 0.310322\n",
            "\tTraining batch 16 Loss: 0.320212\n",
            "\tTraining batch 17 Loss: 0.369093\n",
            "\tTraining batch 18 Loss: 0.291968\n",
            "\tTraining batch 19 Loss: 0.236589\n",
            "\tTraining batch 20 Loss: 0.362817\n",
            "\tTraining batch 21 Loss: 0.198993\n",
            "\tTraining batch 22 Loss: 0.474199\n",
            "\tTraining batch 23 Loss: 0.470186\n",
            "\tTraining batch 24 Loss: 0.333856\n",
            "\tTraining batch 25 Loss: 0.140932\n",
            "\tTraining batch 26 Loss: 0.447461\n",
            "\tTraining batch 27 Loss: 0.305522\n",
            "\tTraining batch 28 Loss: 0.444628\n",
            "\tTraining batch 29 Loss: 0.186797\n",
            "\tTraining batch 30 Loss: 0.171258\n",
            "\tTraining batch 31 Loss: 0.338575\n",
            "\tTraining batch 32 Loss: 0.255657\n",
            "\tTraining batch 33 Loss: 0.411222\n",
            "\tTraining batch 34 Loss: 0.407621\n",
            "\tTraining batch 35 Loss: 0.403982\n",
            "\tTraining batch 36 Loss: 0.455931\n",
            "\tTraining batch 37 Loss: 0.261333\n",
            "\tTraining batch 38 Loss: 0.220434\n",
            "\tTraining batch 39 Loss: 0.314906\n",
            "\tTraining batch 40 Loss: 0.135903\n",
            "\tTraining batch 41 Loss: 0.347424\n",
            "\tTraining batch 42 Loss: 0.218277\n",
            "\tTraining batch 43 Loss: 0.187527\n",
            "\tTraining batch 44 Loss: 0.223010\n",
            "\tTraining batch 45 Loss: 0.408169\n",
            "\tTraining batch 46 Loss: 0.305301\n",
            "\tTraining batch 47 Loss: 0.495159\n",
            "\tTraining batch 48 Loss: 0.355998\n",
            "\tTraining batch 49 Loss: 0.242773\n",
            "\tTraining batch 50 Loss: 0.302908\n",
            "\tTraining batch 51 Loss: 0.318913\n",
            "\tTraining batch 52 Loss: 0.347447\n",
            "\tTraining batch 53 Loss: 0.333900\n",
            "\tTraining batch 54 Loss: 0.226161\n",
            "\tTraining batch 55 Loss: 0.402251\n",
            "\tTraining batch 56 Loss: 0.306078\n",
            "\tTraining batch 57 Loss: 0.175253\n",
            "\tTraining batch 58 Loss: 0.157127\n",
            "\tTraining batch 59 Loss: 0.545715\n",
            "\tTraining batch 60 Loss: 0.248482\n",
            "\tTraining batch 61 Loss: 0.392986\n",
            "\tTraining batch 62 Loss: 0.221075\n",
            "\tTraining batch 63 Loss: 0.354818\n",
            "\tTraining batch 64 Loss: 0.379937\n",
            "\tTraining batch 65 Loss: 0.236721\n",
            "\tTraining batch 66 Loss: 0.378101\n",
            "\tTraining batch 67 Loss: 0.329937\n",
            "\tTraining batch 68 Loss: 0.349437\n",
            "\tTraining batch 69 Loss: 0.293554\n",
            "\tTraining batch 70 Loss: 0.317618\n",
            "\tTraining batch 71 Loss: 0.378847\n",
            "\tTraining batch 72 Loss: 0.375960\n",
            "\tTraining batch 73 Loss: 0.334709\n",
            "\tTraining batch 74 Loss: 0.362790\n",
            "\tTraining batch 75 Loss: 0.282640\n",
            "\tTraining batch 76 Loss: 0.173503\n",
            "\tTraining batch 77 Loss: 0.274533\n",
            "\tTraining batch 78 Loss: 0.324081\n",
            "\tTraining batch 79 Loss: 0.339050\n",
            "\tTraining batch 80 Loss: 0.295713\n",
            "\tTraining batch 81 Loss: 0.458074\n",
            "\tTraining batch 82 Loss: 0.232456\n",
            "\tTraining batch 83 Loss: 0.223666\n",
            "\tTraining batch 84 Loss: 0.225952\n",
            "\tTraining batch 85 Loss: 0.113321\n",
            "\tTraining batch 86 Loss: 0.370550\n",
            "\tTraining batch 87 Loss: 0.230040\n",
            "\tTraining batch 88 Loss: 0.225145\n",
            "\tTraining batch 89 Loss: 0.340603\n",
            "\tTraining batch 90 Loss: 0.228390\n",
            "\tTraining batch 91 Loss: 0.281493\n",
            "\tTraining batch 92 Loss: 0.200389\n",
            "\tTraining batch 93 Loss: 0.212637\n",
            "\tTraining batch 94 Loss: 0.267695\n",
            "\tTraining batch 95 Loss: 0.363374\n",
            "\tTraining batch 96 Loss: 0.390601\n",
            "\tTraining batch 97 Loss: 0.289855\n",
            "Training set: Average loss: 0.304345\n",
            "Validation set: Average loss: 0.261262, Accuracy: 1844/2070 (89%)\n",
            "\n",
            "Epoch: 6\n",
            "\tTraining batch 1 Loss: 0.202992\n",
            "\tTraining batch 2 Loss: 0.185127\n",
            "\tTraining batch 3 Loss: 0.200265\n",
            "\tTraining batch 4 Loss: 0.317494\n",
            "\tTraining batch 5 Loss: 0.260599\n",
            "\tTraining batch 6 Loss: 0.224442\n",
            "\tTraining batch 7 Loss: 0.229342\n",
            "\tTraining batch 8 Loss: 0.488413\n",
            "\tTraining batch 9 Loss: 0.235587\n",
            "\tTraining batch 10 Loss: 0.215883\n",
            "\tTraining batch 11 Loss: 0.438622\n",
            "\tTraining batch 12 Loss: 0.211668\n",
            "\tTraining batch 13 Loss: 0.206439\n",
            "\tTraining batch 14 Loss: 0.477069\n",
            "\tTraining batch 15 Loss: 0.242758\n",
            "\tTraining batch 16 Loss: 0.197952\n",
            "\tTraining batch 17 Loss: 0.278716\n",
            "\tTraining batch 18 Loss: 0.243162\n",
            "\tTraining batch 19 Loss: 0.290456\n",
            "\tTraining batch 20 Loss: 0.297101\n",
            "\tTraining batch 21 Loss: 0.172193\n",
            "\tTraining batch 22 Loss: 0.298947\n",
            "\tTraining batch 23 Loss: 0.438741\n",
            "\tTraining batch 24 Loss: 0.350653\n",
            "\tTraining batch 25 Loss: 0.354331\n",
            "\tTraining batch 26 Loss: 0.302904\n",
            "\tTraining batch 27 Loss: 0.424060\n",
            "\tTraining batch 28 Loss: 0.321812\n",
            "\tTraining batch 29 Loss: 0.240928\n",
            "\tTraining batch 30 Loss: 0.358101\n",
            "\tTraining batch 31 Loss: 0.481351\n",
            "\tTraining batch 32 Loss: 0.306467\n",
            "\tTraining batch 33 Loss: 0.157508\n",
            "\tTraining batch 34 Loss: 0.236418\n",
            "\tTraining batch 35 Loss: 0.120853\n",
            "\tTraining batch 36 Loss: 0.610059\n",
            "\tTraining batch 37 Loss: 0.232965\n",
            "\tTraining batch 38 Loss: 0.167843\n",
            "\tTraining batch 39 Loss: 0.429992\n",
            "\tTraining batch 40 Loss: 0.353130\n",
            "\tTraining batch 41 Loss: 0.316966\n",
            "\tTraining batch 42 Loss: 0.260550\n",
            "\tTraining batch 43 Loss: 0.155492\n",
            "\tTraining batch 44 Loss: 0.208569\n",
            "\tTraining batch 45 Loss: 0.327597\n",
            "\tTraining batch 46 Loss: 0.193102\n",
            "\tTraining batch 47 Loss: 0.470140\n",
            "\tTraining batch 48 Loss: 0.549775\n",
            "\tTraining batch 49 Loss: 0.225487\n",
            "\tTraining batch 50 Loss: 0.282781\n",
            "\tTraining batch 51 Loss: 0.226848\n",
            "\tTraining batch 52 Loss: 0.172132\n",
            "\tTraining batch 53 Loss: 0.496341\n",
            "\tTraining batch 54 Loss: 0.349627\n",
            "\tTraining batch 55 Loss: 0.335082\n",
            "\tTraining batch 56 Loss: 0.374128\n",
            "\tTraining batch 57 Loss: 0.311364\n",
            "\tTraining batch 58 Loss: 0.184913\n",
            "\tTraining batch 59 Loss: 0.393010\n",
            "\tTraining batch 60 Loss: 0.306330\n",
            "\tTraining batch 61 Loss: 0.408629\n",
            "\tTraining batch 62 Loss: 0.257428\n",
            "\tTraining batch 63 Loss: 0.205305\n",
            "\tTraining batch 64 Loss: 0.281618\n",
            "\tTraining batch 65 Loss: 0.249859\n",
            "\tTraining batch 66 Loss: 0.304629\n",
            "\tTraining batch 67 Loss: 0.259181\n",
            "\tTraining batch 68 Loss: 0.149839\n",
            "\tTraining batch 69 Loss: 0.324900\n",
            "\tTraining batch 70 Loss: 0.467976\n",
            "\tTraining batch 71 Loss: 0.299656\n",
            "\tTraining batch 72 Loss: 0.194504\n",
            "\tTraining batch 73 Loss: 0.372642\n",
            "\tTraining batch 74 Loss: 0.181911\n",
            "\tTraining batch 75 Loss: 0.135939\n",
            "\tTraining batch 76 Loss: 0.303734\n",
            "\tTraining batch 77 Loss: 0.341618\n",
            "\tTraining batch 78 Loss: 0.311877\n",
            "\tTraining batch 79 Loss: 0.283188\n",
            "\tTraining batch 80 Loss: 0.203281\n",
            "\tTraining batch 81 Loss: 0.366895\n",
            "\tTraining batch 82 Loss: 0.219809\n",
            "\tTraining batch 83 Loss: 0.280448\n",
            "\tTraining batch 84 Loss: 0.197405\n",
            "\tTraining batch 85 Loss: 0.185230\n",
            "\tTraining batch 86 Loss: 0.269382\n",
            "\tTraining batch 87 Loss: 0.306454\n",
            "\tTraining batch 88 Loss: 0.204962\n",
            "\tTraining batch 89 Loss: 0.476086\n",
            "\tTraining batch 90 Loss: 0.205005\n",
            "\tTraining batch 91 Loss: 0.299282\n",
            "\tTraining batch 92 Loss: 0.139103\n",
            "\tTraining batch 93 Loss: 0.281931\n",
            "\tTraining batch 94 Loss: 0.353748\n",
            "\tTraining batch 95 Loss: 0.197674\n",
            "\tTraining batch 96 Loss: 0.263573\n",
            "\tTraining batch 97 Loss: 0.213019\n",
            "Training set: Average loss: 0.288055\n",
            "Validation set: Average loss: 0.266775, Accuracy: 1837/2070 (89%)\n",
            "\n",
            "Epoch: 7\n",
            "\tTraining batch 1 Loss: 0.287332\n",
            "\tTraining batch 2 Loss: 0.307781\n",
            "\tTraining batch 3 Loss: 0.406652\n",
            "\tTraining batch 4 Loss: 0.186304\n",
            "\tTraining batch 5 Loss: 0.118614\n",
            "\tTraining batch 6 Loss: 0.354593\n",
            "\tTraining batch 7 Loss: 0.246603\n",
            "\tTraining batch 8 Loss: 0.389256\n",
            "\tTraining batch 9 Loss: 0.293575\n",
            "\tTraining batch 10 Loss: 0.222585\n",
            "\tTraining batch 11 Loss: 0.298508\n",
            "\tTraining batch 12 Loss: 0.308117\n",
            "\tTraining batch 13 Loss: 0.170130\n",
            "\tTraining batch 14 Loss: 0.255889\n",
            "\tTraining batch 15 Loss: 0.352147\n",
            "\tTraining batch 16 Loss: 0.250927\n",
            "\tTraining batch 17 Loss: 0.242416\n",
            "\tTraining batch 18 Loss: 0.227416\n",
            "\tTraining batch 19 Loss: 0.229106\n",
            "\tTraining batch 20 Loss: 0.288099\n",
            "\tTraining batch 21 Loss: 0.151335\n",
            "\tTraining batch 22 Loss: 0.469220\n",
            "\tTraining batch 23 Loss: 0.272611\n",
            "\tTraining batch 24 Loss: 0.365827\n",
            "\tTraining batch 25 Loss: 0.225299\n",
            "\tTraining batch 26 Loss: 0.398052\n",
            "\tTraining batch 27 Loss: 0.305274\n",
            "\tTraining batch 28 Loss: 0.311325\n",
            "\tTraining batch 29 Loss: 0.146838\n",
            "\tTraining batch 30 Loss: 0.357070\n",
            "\tTraining batch 31 Loss: 0.211870\n",
            "\tTraining batch 32 Loss: 0.288292\n",
            "\tTraining batch 33 Loss: 0.354891\n",
            "\tTraining batch 34 Loss: 0.310299\n",
            "\tTraining batch 35 Loss: 0.189292\n",
            "\tTraining batch 36 Loss: 0.277522\n",
            "\tTraining batch 37 Loss: 0.195888\n",
            "\tTraining batch 38 Loss: 0.204973\n",
            "\tTraining batch 39 Loss: 0.389440\n",
            "\tTraining batch 40 Loss: 0.285190\n",
            "\tTraining batch 41 Loss: 0.214237\n",
            "\tTraining batch 42 Loss: 0.147307\n",
            "\tTraining batch 43 Loss: 0.089274\n",
            "\tTraining batch 44 Loss: 0.193306\n",
            "\tTraining batch 45 Loss: 0.327773\n",
            "\tTraining batch 46 Loss: 0.270740\n",
            "\tTraining batch 47 Loss: 0.388370\n",
            "\tTraining batch 48 Loss: 0.452345\n",
            "\tTraining batch 49 Loss: 0.369845\n",
            "\tTraining batch 50 Loss: 0.336928\n",
            "\tTraining batch 51 Loss: 0.195896\n",
            "\tTraining batch 52 Loss: 0.232785\n",
            "\tTraining batch 53 Loss: 0.485740\n",
            "\tTraining batch 54 Loss: 0.288702\n",
            "\tTraining batch 55 Loss: 0.355004\n",
            "\tTraining batch 56 Loss: 0.259225\n",
            "\tTraining batch 57 Loss: 0.125226\n",
            "\tTraining batch 58 Loss: 0.138772\n",
            "\tTraining batch 59 Loss: 0.460206\n",
            "\tTraining batch 60 Loss: 0.332307\n",
            "\tTraining batch 61 Loss: 0.390148\n",
            "\tTraining batch 62 Loss: 0.207264\n",
            "\tTraining batch 63 Loss: 0.246456\n",
            "\tTraining batch 64 Loss: 0.242254\n",
            "\tTraining batch 65 Loss: 0.376053\n",
            "\tTraining batch 66 Loss: 0.267125\n",
            "\tTraining batch 67 Loss: 0.337417\n",
            "\tTraining batch 68 Loss: 0.461393\n",
            "\tTraining batch 69 Loss: 0.331036\n",
            "\tTraining batch 70 Loss: 0.220369\n",
            "\tTraining batch 71 Loss: 0.352858\n",
            "\tTraining batch 72 Loss: 0.414173\n",
            "\tTraining batch 73 Loss: 0.284932\n",
            "\tTraining batch 74 Loss: 0.437363\n",
            "\tTraining batch 75 Loss: 0.258571\n",
            "\tTraining batch 76 Loss: 0.181790\n",
            "\tTraining batch 77 Loss: 0.582626\n",
            "\tTraining batch 78 Loss: 0.447252\n",
            "\tTraining batch 79 Loss: 0.269222\n",
            "\tTraining batch 80 Loss: 0.218789\n",
            "\tTraining batch 81 Loss: 0.501251\n",
            "\tTraining batch 82 Loss: 0.184203\n",
            "\tTraining batch 83 Loss: 0.314128\n",
            "\tTraining batch 84 Loss: 0.137152\n",
            "\tTraining batch 85 Loss: 0.188816\n",
            "\tTraining batch 86 Loss: 0.314639\n",
            "\tTraining batch 87 Loss: 0.321774\n",
            "\tTraining batch 88 Loss: 0.187940\n",
            "\tTraining batch 89 Loss: 0.306627\n",
            "\tTraining batch 90 Loss: 0.215649\n",
            "\tTraining batch 91 Loss: 0.288030\n",
            "\tTraining batch 92 Loss: 0.174122\n",
            "\tTraining batch 93 Loss: 0.345180\n",
            "\tTraining batch 94 Loss: 0.193958\n",
            "\tTraining batch 95 Loss: 0.167807\n",
            "\tTraining batch 96 Loss: 0.203753\n",
            "\tTraining batch 97 Loss: 0.184460\n",
            "Training set: Average loss: 0.284197\n",
            "Validation set: Average loss: 0.257151, Accuracy: 1847/2070 (89%)\n",
            "\n",
            "Epoch: 8\n",
            "\tTraining batch 1 Loss: 0.150786\n",
            "\tTraining batch 2 Loss: 0.218062\n",
            "\tTraining batch 3 Loss: 0.302412\n",
            "\tTraining batch 4 Loss: 0.434880\n",
            "\tTraining batch 5 Loss: 0.172900\n",
            "\tTraining batch 6 Loss: 0.245993\n",
            "\tTraining batch 7 Loss: 0.206024\n",
            "\tTraining batch 8 Loss: 0.393480\n",
            "\tTraining batch 9 Loss: 0.226619\n",
            "\tTraining batch 10 Loss: 0.289108\n",
            "\tTraining batch 11 Loss: 0.291498\n",
            "\tTraining batch 12 Loss: 0.175394\n",
            "\tTraining batch 13 Loss: 0.159720\n",
            "\tTraining batch 14 Loss: 0.512355\n",
            "\tTraining batch 15 Loss: 0.274840\n",
            "\tTraining batch 16 Loss: 0.322346\n",
            "\tTraining batch 17 Loss: 0.347437\n",
            "\tTraining batch 18 Loss: 0.190171\n",
            "\tTraining batch 19 Loss: 0.212836\n",
            "\tTraining batch 20 Loss: 0.461790\n",
            "\tTraining batch 21 Loss: 0.289194\n",
            "\tTraining batch 22 Loss: 0.317578\n",
            "\tTraining batch 23 Loss: 0.417112\n",
            "\tTraining batch 24 Loss: 0.225709\n",
            "\tTraining batch 25 Loss: 0.230034\n",
            "\tTraining batch 26 Loss: 0.470651\n",
            "\tTraining batch 27 Loss: 0.446116\n",
            "\tTraining batch 28 Loss: 0.316128\n",
            "\tTraining batch 29 Loss: 0.213437\n",
            "\tTraining batch 30 Loss: 0.328847\n",
            "\tTraining batch 31 Loss: 0.402162\n",
            "\tTraining batch 32 Loss: 0.242423\n",
            "\tTraining batch 33 Loss: 0.277449\n",
            "\tTraining batch 34 Loss: 0.350737\n",
            "\tTraining batch 35 Loss: 0.122280\n",
            "\tTraining batch 36 Loss: 0.287759\n",
            "\tTraining batch 37 Loss: 0.148211\n",
            "\tTraining batch 38 Loss: 0.285361\n",
            "\tTraining batch 39 Loss: 0.430991\n",
            "\tTraining batch 40 Loss: 0.161527\n",
            "\tTraining batch 41 Loss: 0.482403\n",
            "\tTraining batch 42 Loss: 0.171588\n",
            "\tTraining batch 43 Loss: 0.315214\n",
            "\tTraining batch 44 Loss: 0.196756\n",
            "\tTraining batch 45 Loss: 0.184501\n",
            "\tTraining batch 46 Loss: 0.209639\n",
            "\tTraining batch 47 Loss: 0.396201\n",
            "\tTraining batch 48 Loss: 0.448036\n",
            "\tTraining batch 49 Loss: 0.226779\n",
            "\tTraining batch 50 Loss: 0.258427\n",
            "\tTraining batch 51 Loss: 0.274830\n",
            "\tTraining batch 52 Loss: 0.470517\n",
            "\tTraining batch 53 Loss: 0.487366\n",
            "\tTraining batch 54 Loss: 0.352199\n",
            "\tTraining batch 55 Loss: 0.311718\n",
            "\tTraining batch 56 Loss: 0.246397\n",
            "\tTraining batch 57 Loss: 0.245001\n",
            "\tTraining batch 58 Loss: 0.160829\n",
            "\tTraining batch 59 Loss: 0.377290\n",
            "\tTraining batch 60 Loss: 0.387986\n",
            "\tTraining batch 61 Loss: 0.392744\n",
            "\tTraining batch 62 Loss: 0.266808\n",
            "\tTraining batch 63 Loss: 0.130909\n",
            "\tTraining batch 64 Loss: 0.282522\n",
            "\tTraining batch 65 Loss: 0.266150\n",
            "\tTraining batch 66 Loss: 0.327536\n",
            "\tTraining batch 67 Loss: 0.201058\n",
            "\tTraining batch 68 Loss: 0.176070\n",
            "\tTraining batch 69 Loss: 0.258322\n",
            "\tTraining batch 70 Loss: 0.204103\n",
            "\tTraining batch 71 Loss: 0.236810\n",
            "\tTraining batch 72 Loss: 0.414039\n",
            "\tTraining batch 73 Loss: 0.299574\n",
            "\tTraining batch 74 Loss: 0.193893\n",
            "\tTraining batch 75 Loss: 0.276958\n",
            "\tTraining batch 76 Loss: 0.204031\n",
            "\tTraining batch 77 Loss: 0.435230\n",
            "\tTraining batch 78 Loss: 0.338600\n",
            "\tTraining batch 79 Loss: 0.218683\n",
            "\tTraining batch 80 Loss: 0.215857\n",
            "\tTraining batch 81 Loss: 0.267144\n",
            "\tTraining batch 82 Loss: 0.159110\n",
            "\tTraining batch 83 Loss: 0.225185\n",
            "\tTraining batch 84 Loss: 0.108106\n",
            "\tTraining batch 85 Loss: 0.150708\n",
            "\tTraining batch 86 Loss: 0.204545\n",
            "\tTraining batch 87 Loss: 0.360969\n",
            "\tTraining batch 88 Loss: 0.162063\n",
            "\tTraining batch 89 Loss: 0.343108\n",
            "\tTraining batch 90 Loss: 0.318398\n",
            "\tTraining batch 91 Loss: 0.405905\n",
            "\tTraining batch 92 Loss: 0.082024\n",
            "\tTraining batch 93 Loss: 0.328257\n",
            "\tTraining batch 94 Loss: 0.328983\n",
            "\tTraining batch 95 Loss: 0.271974\n",
            "\tTraining batch 96 Loss: 0.176444\n",
            "\tTraining batch 97 Loss: 0.326881\n",
            "Training set: Average loss: 0.281606\n",
            "Validation set: Average loss: 0.297829, Accuracy: 1827/2070 (88%)\n",
            "\n",
            "Epoch: 9\n",
            "\tTraining batch 1 Loss: 0.303474\n",
            "\tTraining batch 2 Loss: 0.180112\n",
            "\tTraining batch 3 Loss: 0.290475\n",
            "\tTraining batch 4 Loss: 0.260162\n",
            "\tTraining batch 5 Loss: 0.255518\n",
            "\tTraining batch 6 Loss: 0.225994\n",
            "\tTraining batch 7 Loss: 0.202430\n",
            "\tTraining batch 8 Loss: 0.420845\n",
            "\tTraining batch 9 Loss: 0.312423\n",
            "\tTraining batch 10 Loss: 0.236443\n",
            "\tTraining batch 11 Loss: 0.219806\n",
            "\tTraining batch 12 Loss: 0.330593\n",
            "\tTraining batch 13 Loss: 0.224269\n",
            "\tTraining batch 14 Loss: 0.270512\n",
            "\tTraining batch 15 Loss: 0.299300\n",
            "\tTraining batch 16 Loss: 0.378972\n",
            "\tTraining batch 17 Loss: 0.290931\n",
            "\tTraining batch 18 Loss: 0.248370\n",
            "\tTraining batch 19 Loss: 0.241585\n",
            "\tTraining batch 20 Loss: 0.240134\n",
            "\tTraining batch 21 Loss: 0.116138\n",
            "\tTraining batch 22 Loss: 0.399937\n",
            "\tTraining batch 23 Loss: 0.378227\n",
            "\tTraining batch 24 Loss: 0.345966\n",
            "\tTraining batch 25 Loss: 0.189097\n",
            "\tTraining batch 26 Loss: 0.418549\n",
            "\tTraining batch 27 Loss: 0.264888\n",
            "\tTraining batch 28 Loss: 0.306343\n",
            "\tTraining batch 29 Loss: 0.186752\n",
            "\tTraining batch 30 Loss: 0.255331\n",
            "\tTraining batch 31 Loss: 0.243066\n",
            "\tTraining batch 32 Loss: 0.287439\n",
            "\tTraining batch 33 Loss: 0.196368\n",
            "\tTraining batch 34 Loss: 0.265655\n",
            "\tTraining batch 35 Loss: 0.190121\n",
            "\tTraining batch 36 Loss: 0.472427\n",
            "\tTraining batch 37 Loss: 0.144537\n",
            "\tTraining batch 38 Loss: 0.178828\n",
            "\tTraining batch 39 Loss: 0.296515\n",
            "\tTraining batch 40 Loss: 0.262817\n",
            "\tTraining batch 41 Loss: 0.257007\n",
            "\tTraining batch 42 Loss: 0.240364\n",
            "\tTraining batch 43 Loss: 0.274543\n",
            "\tTraining batch 44 Loss: 0.349499\n",
            "\tTraining batch 45 Loss: 0.310088\n",
            "\tTraining batch 46 Loss: 0.203155\n",
            "\tTraining batch 47 Loss: 0.419116\n",
            "\tTraining batch 48 Loss: 0.283819\n",
            "\tTraining batch 49 Loss: 0.343582\n",
            "\tTraining batch 50 Loss: 0.251214\n",
            "\tTraining batch 51 Loss: 0.271082\n",
            "\tTraining batch 52 Loss: 0.439266\n",
            "\tTraining batch 53 Loss: 0.518233\n",
            "\tTraining batch 54 Loss: 0.250996\n",
            "\tTraining batch 55 Loss: 0.300132\n",
            "\tTraining batch 56 Loss: 0.362463\n",
            "\tTraining batch 57 Loss: 0.230570\n",
            "\tTraining batch 58 Loss: 0.206518\n",
            "\tTraining batch 59 Loss: 0.448150\n",
            "\tTraining batch 60 Loss: 0.230649\n",
            "\tTraining batch 61 Loss: 0.335172\n",
            "\tTraining batch 62 Loss: 0.159660\n",
            "\tTraining batch 63 Loss: 0.309177\n",
            "\tTraining batch 64 Loss: 0.272080\n",
            "\tTraining batch 65 Loss: 0.240942\n",
            "\tTraining batch 66 Loss: 0.373584\n",
            "\tTraining batch 67 Loss: 0.232375\n",
            "\tTraining batch 68 Loss: 0.268991\n",
            "\tTraining batch 69 Loss: 0.239778\n",
            "\tTraining batch 70 Loss: 0.191389\n",
            "\tTraining batch 71 Loss: 0.445350\n",
            "\tTraining batch 72 Loss: 0.247675\n",
            "\tTraining batch 73 Loss: 0.249420\n",
            "\tTraining batch 74 Loss: 0.282384\n",
            "\tTraining batch 75 Loss: 0.215205\n",
            "\tTraining batch 76 Loss: 0.246946\n",
            "\tTraining batch 77 Loss: 0.255598\n",
            "\tTraining batch 78 Loss: 0.263632\n",
            "\tTraining batch 79 Loss: 0.251690\n",
            "\tTraining batch 80 Loss: 0.235081\n",
            "\tTraining batch 81 Loss: 0.375505\n",
            "\tTraining batch 82 Loss: 0.260003\n",
            "\tTraining batch 83 Loss: 0.352583\n",
            "\tTraining batch 84 Loss: 0.082680\n",
            "\tTraining batch 85 Loss: 0.264253\n",
            "\tTraining batch 86 Loss: 0.248475\n",
            "\tTraining batch 87 Loss: 0.327556\n",
            "\tTraining batch 88 Loss: 0.277794\n",
            "\tTraining batch 89 Loss: 0.283655\n",
            "\tTraining batch 90 Loss: 0.241311\n",
            "\tTraining batch 91 Loss: 0.283948\n",
            "\tTraining batch 92 Loss: 0.192529\n",
            "\tTraining batch 93 Loss: 0.412820\n",
            "\tTraining batch 94 Loss: 0.183364\n",
            "\tTraining batch 95 Loss: 0.171934\n",
            "\tTraining batch 96 Loss: 0.241002\n",
            "\tTraining batch 97 Loss: 0.283642\n",
            "Training set: Average loss: 0.276505\n",
            "Validation set: Average loss: 0.268539, Accuracy: 1853/2070 (90%)\n",
            "\n",
            "Epoch: 10\n",
            "\tTraining batch 1 Loss: 0.256552\n",
            "\tTraining batch 2 Loss: 0.277665\n",
            "\tTraining batch 3 Loss: 0.371196\n",
            "\tTraining batch 4 Loss: 0.259750\n",
            "\tTraining batch 5 Loss: 0.177695\n",
            "\tTraining batch 6 Loss: 0.243364\n",
            "\tTraining batch 7 Loss: 0.221827\n",
            "\tTraining batch 8 Loss: 0.362315\n",
            "\tTraining batch 9 Loss: 0.191053\n",
            "\tTraining batch 10 Loss: 0.198967\n",
            "\tTraining batch 11 Loss: 0.244452\n",
            "\tTraining batch 12 Loss: 0.339923\n",
            "\tTraining batch 13 Loss: 0.241270\n",
            "\tTraining batch 14 Loss: 0.213669\n",
            "\tTraining batch 15 Loss: 0.314441\n",
            "\tTraining batch 16 Loss: 0.271592\n",
            "\tTraining batch 17 Loss: 0.311054\n",
            "\tTraining batch 18 Loss: 0.250080\n",
            "\tTraining batch 19 Loss: 0.337135\n",
            "\tTraining batch 20 Loss: 0.244453\n",
            "\tTraining batch 21 Loss: 0.214220\n",
            "\tTraining batch 22 Loss: 0.457922\n",
            "\tTraining batch 23 Loss: 0.407007\n",
            "\tTraining batch 24 Loss: 0.334117\n",
            "\tTraining batch 25 Loss: 0.275229\n",
            "\tTraining batch 26 Loss: 0.283150\n",
            "\tTraining batch 27 Loss: 0.222227\n",
            "\tTraining batch 28 Loss: 0.333721\n",
            "\tTraining batch 29 Loss: 0.251266\n",
            "\tTraining batch 30 Loss: 0.343995\n",
            "\tTraining batch 31 Loss: 0.240108\n",
            "\tTraining batch 32 Loss: 0.151909\n",
            "\tTraining batch 33 Loss: 0.284326\n",
            "\tTraining batch 34 Loss: 0.348609\n",
            "\tTraining batch 35 Loss: 0.295026\n",
            "\tTraining batch 36 Loss: 0.381754\n",
            "\tTraining batch 37 Loss: 0.287323\n",
            "\tTraining batch 38 Loss: 0.258291\n",
            "\tTraining batch 39 Loss: 0.287313\n",
            "\tTraining batch 40 Loss: 0.250249\n",
            "\tTraining batch 41 Loss: 0.384035\n",
            "\tTraining batch 42 Loss: 0.285144\n",
            "\tTraining batch 43 Loss: 0.173163\n",
            "\tTraining batch 44 Loss: 0.210336\n",
            "\tTraining batch 45 Loss: 0.319180\n",
            "\tTraining batch 46 Loss: 0.167396\n",
            "\tTraining batch 47 Loss: 0.346009\n",
            "\tTraining batch 48 Loss: 0.316399\n",
            "\tTraining batch 49 Loss: 0.254362\n",
            "\tTraining batch 50 Loss: 0.267434\n",
            "\tTraining batch 51 Loss: 0.313971\n",
            "\tTraining batch 52 Loss: 0.292695\n",
            "\tTraining batch 53 Loss: 0.521185\n",
            "\tTraining batch 54 Loss: 0.375270\n",
            "\tTraining batch 55 Loss: 0.377329\n",
            "\tTraining batch 56 Loss: 0.404574\n",
            "\tTraining batch 57 Loss: 0.156820\n",
            "\tTraining batch 58 Loss: 0.140445\n",
            "\tTraining batch 59 Loss: 0.371543\n",
            "\tTraining batch 60 Loss: 0.257344\n",
            "\tTraining batch 61 Loss: 0.439362\n",
            "\tTraining batch 62 Loss: 0.177520\n",
            "\tTraining batch 63 Loss: 0.338472\n",
            "\tTraining batch 64 Loss: 0.344273\n",
            "\tTraining batch 65 Loss: 0.273731\n",
            "\tTraining batch 66 Loss: 0.254769\n",
            "\tTraining batch 67 Loss: 0.194062\n",
            "\tTraining batch 68 Loss: 0.290815\n",
            "\tTraining batch 69 Loss: 0.287370\n",
            "\tTraining batch 70 Loss: 0.330671\n",
            "\tTraining batch 71 Loss: 0.414598\n",
            "\tTraining batch 72 Loss: 0.422282\n",
            "\tTraining batch 73 Loss: 0.327267\n",
            "\tTraining batch 74 Loss: 0.151157\n",
            "\tTraining batch 75 Loss: 0.225146\n",
            "\tTraining batch 76 Loss: 0.242818\n",
            "\tTraining batch 77 Loss: 0.279342\n",
            "\tTraining batch 78 Loss: 0.291669\n",
            "\tTraining batch 79 Loss: 0.196575\n",
            "\tTraining batch 80 Loss: 0.275824\n",
            "\tTraining batch 81 Loss: 0.314042\n",
            "\tTraining batch 82 Loss: 0.144787\n",
            "\tTraining batch 83 Loss: 0.327011\n",
            "\tTraining batch 84 Loss: 0.233003\n",
            "\tTraining batch 85 Loss: 0.147416\n",
            "\tTraining batch 86 Loss: 0.241256\n",
            "\tTraining batch 87 Loss: 0.250847\n",
            "\tTraining batch 88 Loss: 0.156115\n",
            "\tTraining batch 89 Loss: 0.394556\n",
            "\tTraining batch 90 Loss: 0.294581\n",
            "\tTraining batch 91 Loss: 0.292210\n",
            "\tTraining batch 92 Loss: 0.105365\n",
            "\tTraining batch 93 Loss: 0.265292\n",
            "\tTraining batch 94 Loss: 0.115586\n",
            "\tTraining batch 95 Loss: 0.178074\n",
            "\tTraining batch 96 Loss: 0.165241\n",
            "\tTraining batch 97 Loss: 0.312843\n",
            "Training set: Average loss: 0.276977\n",
            "Validation set: Average loss: 0.274173, Accuracy: 1837/2070 (89%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#L1 Regularization\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(28 * 28 * 1, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 10)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)\n",
        "  \n",
        "  def compute_l1_loss(self, w):\n",
        "      return torch.abs(w).sum()\n",
        "  \n",
        "  \n",
        "if __name__ == '__main__':\n",
        "  \n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "  \n",
        "  # Prepare CIFAR-10 dataset\n",
        "  dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
        "  trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "  \n",
        "  # Initialize the MLP\n",
        "  mlp = MLP()\n",
        "  \n",
        "  # Define the loss function and optimizer\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "  \n",
        "  # Run the training loop\n",
        "  for epoch in range(0, 5): # 5 epochs at maximum\n",
        "    \n",
        "    # Print epoch\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    \n",
        "    # Iterate over the DataLoader for training data\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "      \n",
        "      # Get inputs\n",
        "      inputs, targets = data\n",
        "      \n",
        "      # Zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # Perform forward pass\n",
        "      outputs = mlp(inputs)\n",
        "      \n",
        "      # Compute loss\n",
        "      loss = loss_function(outputs, targets)\n",
        "      \n",
        "      # Compute L1 loss component\n",
        "      l1_weight = 1.0\n",
        "      l1_parameters = []\n",
        "      for parameter in mlp.parameters():\n",
        "          l1_parameters.append(parameter.view(-1))\n",
        "      l1 = l1_weight * mlp.compute_l1_loss(torch.cat(l1_parameters))\n",
        "      \n",
        "      # Add L1 loss component\n",
        "      loss += l1\n",
        "      \n",
        "      # Perform backward pass\n",
        "      loss.backward()\n",
        "      \n",
        "      # Perform optimization\n",
        "      optimizer.step()\n",
        "      \n",
        "      # Print statistics\n",
        "      minibatch_loss = loss.item()\n",
        "      if i % 500 == 499:\n",
        "          print('Loss after mini-batch %5d: %.5f (of which %.5f L1 loss)' %\n",
        "                (i + 1, minibatch_loss, l1))\n",
        "          current_loss = 0.0\n",
        "\n",
        "  # Process is complete.\n",
        "  print('Training process has finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd3MZL5QIJbV",
        "outputId": "1739cef4-b8a7-4964-f71f-8422541423ec"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Loss after mini-batch   500: 65.46844 (of which 63.18564 L1 loss)\n",
            "Loss after mini-batch  1000: 13.96671 (of which 11.65112 L1 loss)\n",
            "Loss after mini-batch  1500: 3.68985 (of which 1.38492 L1 loss)\n",
            "Loss after mini-batch  2000: 2.93928 (of which 0.63669 L1 loss)\n",
            "Loss after mini-batch  2500: 2.93661 (of which 0.63402 L1 loss)\n",
            "Loss after mini-batch  3000: 2.94112 (of which 0.63853 L1 loss)\n",
            "Loss after mini-batch  3500: 2.93957 (of which 0.63699 L1 loss)\n",
            "Loss after mini-batch  4000: 2.93565 (of which 0.63306 L1 loss)\n",
            "Loss after mini-batch  4500: 2.94100 (of which 0.63842 L1 loss)\n",
            "Loss after mini-batch  5000: 2.93991 (of which 0.63732 L1 loss)\n",
            "Loss after mini-batch  5500: 2.93886 (of which 0.63627 L1 loss)\n",
            "Loss after mini-batch  6000: 2.93798 (of which 0.63539 L1 loss)\n",
            "Starting epoch 2\n",
            "Loss after mini-batch   500: 2.94104 (of which 0.63846 L1 loss)\n",
            "Loss after mini-batch  1000: 2.93711 (of which 0.63452 L1 loss)\n",
            "Loss after mini-batch  1500: 2.93915 (of which 0.63656 L1 loss)\n",
            "Loss after mini-batch  2000: 2.93973 (of which 0.63715 L1 loss)\n",
            "Loss after mini-batch  2500: 2.93829 (of which 0.63571 L1 loss)\n",
            "Loss after mini-batch  3000: 2.93863 (of which 0.63604 L1 loss)\n",
            "Loss after mini-batch  3500: 2.94125 (of which 0.63867 L1 loss)\n",
            "Loss after mini-batch  4000: 2.93739 (of which 0.63481 L1 loss)\n",
            "Loss after mini-batch  4500: 2.93801 (of which 0.63543 L1 loss)\n",
            "Loss after mini-batch  5000: 2.93742 (of which 0.63484 L1 loss)\n",
            "Loss after mini-batch  5500: 2.93860 (of which 0.63601 L1 loss)\n",
            "Loss after mini-batch  6000: 2.94074 (of which 0.63815 L1 loss)\n",
            "Starting epoch 3\n",
            "Loss after mini-batch   500: 2.94320 (of which 0.64061 L1 loss)\n",
            "Loss after mini-batch  1000: 2.93574 (of which 0.63316 L1 loss)\n",
            "Loss after mini-batch  1500: 2.94111 (of which 0.63853 L1 loss)\n",
            "Loss after mini-batch  2000: 2.94040 (of which 0.63781 L1 loss)\n",
            "Loss after mini-batch  2500: 2.93510 (of which 0.63252 L1 loss)\n",
            "Loss after mini-batch  3000: 2.93989 (of which 0.63731 L1 loss)\n",
            "Loss after mini-batch  3500: 2.94161 (of which 0.63903 L1 loss)\n",
            "Loss after mini-batch  4000: 2.93590 (of which 0.63331 L1 loss)\n",
            "Loss after mini-batch  4500: 2.93946 (of which 0.63687 L1 loss)\n",
            "Loss after mini-batch  5000: 2.94033 (of which 0.63774 L1 loss)\n",
            "Loss after mini-batch  5500: 2.93561 (of which 0.63302 L1 loss)\n",
            "Loss after mini-batch  6000: 2.93805 (of which 0.63547 L1 loss)\n",
            "Starting epoch 4\n",
            "Loss after mini-batch   500: 2.94109 (of which 0.63851 L1 loss)\n",
            "Loss after mini-batch  1000: 2.93917 (of which 0.63659 L1 loss)\n",
            "Loss after mini-batch  1500: 2.94109 (of which 0.63851 L1 loss)\n",
            "Loss after mini-batch  2000: 2.93766 (of which 0.63507 L1 loss)\n",
            "Loss after mini-batch  2500: 2.93921 (of which 0.63662 L1 loss)\n",
            "Loss after mini-batch  3000: 2.93973 (of which 0.63715 L1 loss)\n",
            "Loss after mini-batch  3500: 2.93762 (of which 0.63503 L1 loss)\n",
            "Loss after mini-batch  4000: 2.93735 (of which 0.63477 L1 loss)\n",
            "Loss after mini-batch  4500: 2.94190 (of which 0.63931 L1 loss)\n",
            "Loss after mini-batch  5000: 2.94022 (of which 0.63763 L1 loss)\n",
            "Loss after mini-batch  5500: 2.93607 (of which 0.63349 L1 loss)\n",
            "Loss after mini-batch  6000: 2.94036 (of which 0.63778 L1 loss)\n",
            "Starting epoch 5\n",
            "Loss after mini-batch   500: 2.94215 (of which 0.63957 L1 loss)\n",
            "Loss after mini-batch  1000: 2.93445 (of which 0.63186 L1 loss)\n",
            "Loss after mini-batch  1500: 2.93992 (of which 0.63733 L1 loss)\n",
            "Loss after mini-batch  2000: 2.94098 (of which 0.63839 L1 loss)\n",
            "Loss after mini-batch  2500: 2.93725 (of which 0.63468 L1 loss)\n",
            "Loss after mini-batch  3000: 2.93692 (of which 0.63433 L1 loss)\n",
            "Loss after mini-batch  3500: 2.94259 (of which 0.63999 L1 loss)\n",
            "Loss after mini-batch  4000: 2.93637 (of which 0.63378 L1 loss)\n",
            "Loss after mini-batch  4500: 2.93862 (of which 0.63604 L1 loss)\n",
            "Loss after mini-batch  5000: 2.93913 (of which 0.63653 L1 loss)\n",
            "Loss after mini-batch  5500: 2.93972 (of which 0.63714 L1 loss)\n",
            "Loss after mini-batch  6000: 2.94161 (of which 0.63903 L1 loss)\n",
            "Training process has finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#L2 Regularization\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  '''\n",
        "    Multilayer Perceptron.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(28 * 28 * 1, 64),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(64, 32),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(32, 10)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)\n",
        "  \n",
        "  def compute_l2_loss(self, w):\n",
        "      return torch.square(w).sum()\n",
        "  \n",
        "  \n",
        "if __name__ == '__main__':\n",
        "  \n",
        "  # Set fixed random number seed\n",
        "  torch.manual_seed(42)\n",
        "  \n",
        "  # Prepare CIFAR-10 dataset\n",
        "  dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
        "  trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
        "  \n",
        "  # Initialize the MLP\n",
        "  mlp = MLP()\n",
        "  \n",
        "  # Define the loss function and optimizer\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
        "  \n",
        "  # Run the training loop\n",
        "  for epoch in range(0, 5): # 5 epochs at maximum\n",
        "    \n",
        "    # Print epoch\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    \n",
        "    # Iterate over the DataLoader for training data\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "      \n",
        "      # Get inputs\n",
        "      inputs, targets = data\n",
        "      \n",
        "      # Zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # Perform forward pass\n",
        "      outputs = mlp(inputs)\n",
        "      \n",
        "      # Compute loss\n",
        "      loss = loss_function(outputs, targets)\n",
        "      \n",
        "      # Compute l2 loss component\n",
        "      l2_weight = 1.0\n",
        "      l2_parameters = []\n",
        "      for parameter in mlp.parameters():\n",
        "          l2_parameters.append(parameter.view(-1))\n",
        "      l2 = l2_weight * mlp.compute_l2_loss(torch.cat(l2_parameters))\n",
        "      \n",
        "      # Add l2 loss component\n",
        "      loss += l2\n",
        "      \n",
        "      # Perform backward pass\n",
        "      loss.backward()\n",
        "      \n",
        "      # Perform optimization\n",
        "      optimizer.step()\n",
        "      \n",
        "      # Print statistics\n",
        "      minibatch_loss = loss.item()\n",
        "      if i % 500 == 499:\n",
        "          print('Loss after mini-batch %5d: %.5f (of which %.5f l2 loss)' %\n",
        "                (i + 1, minibatch_loss, l2))\n",
        "          current_loss = 0.0\n",
        "\n",
        "  # Process is complete.\n",
        "  print('Training process has finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6dYkA32OCKl",
        "outputId": "39371781-2f1e-4356-a9bb-03db50167ca3"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Loss after mini-batch   500: 6.90672 (of which 4.62813 l2 loss)\n",
            "Loss after mini-batch  1000: 3.57362 (of which 1.24972 l2 loss)\n",
            "Loss after mini-batch  1500: 2.59496 (of which 0.29211 l2 loss)\n",
            "Loss after mini-batch  2000: 2.35297 (of which 0.05843 l2 loss)\n",
            "Loss after mini-batch  2500: 2.30740 (of which 0.00982 l2 loss)\n",
            "Loss after mini-batch  3000: 2.30312 (of which 0.00131 l2 loss)\n",
            "Loss after mini-batch  3500: 2.30289 (of which 0.00020 l2 loss)\n",
            "Loss after mini-batch  4000: 2.30143 (of which 0.00008 l2 loss)\n",
            "Loss after mini-batch  4500: 2.30402 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  5000: 2.30044 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  5500: 2.30264 (of which 0.00006 l2 loss)\n",
            "Loss after mini-batch  6000: 2.30348 (of which 0.00009 l2 loss)\n",
            "Starting epoch 2\n",
            "Loss after mini-batch   500: 2.30326 (of which 0.00006 l2 loss)\n",
            "Loss after mini-batch  1000: 2.30304 (of which 0.00006 l2 loss)\n",
            "Loss after mini-batch  1500: 2.30264 (of which 0.00004 l2 loss)\n",
            "Loss after mini-batch  2000: 2.30267 (of which 0.00012 l2 loss)\n",
            "Loss after mini-batch  2500: 2.29971 (of which 0.00015 l2 loss)\n",
            "Loss after mini-batch  3000: 2.30323 (of which 0.00008 l2 loss)\n",
            "Loss after mini-batch  3500: 2.30171 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  4000: 2.30261 (of which 0.00013 l2 loss)\n",
            "Loss after mini-batch  4500: 2.30159 (of which 0.00008 l2 loss)\n",
            "Loss after mini-batch  5000: 2.30144 (of which 0.00009 l2 loss)\n",
            "Loss after mini-batch  5500: 2.30351 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  6000: 2.30123 (of which 0.00007 l2 loss)\n",
            "Starting epoch 3\n",
            "Loss after mini-batch   500: 2.30285 (of which 0.00009 l2 loss)\n",
            "Loss after mini-batch  1000: 2.30252 (of which 0.00009 l2 loss)\n",
            "Loss after mini-batch  1500: 2.30218 (of which 0.00008 l2 loss)\n",
            "Loss after mini-batch  2000: 2.30202 (of which 0.00009 l2 loss)\n",
            "Loss after mini-batch  2500: 2.30251 (of which 0.00006 l2 loss)\n",
            "Loss after mini-batch  3000: 2.30207 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  3500: 2.30398 (of which 0.00004 l2 loss)\n",
            "Loss after mini-batch  4000: 2.30179 (of which 0.00006 l2 loss)\n",
            "Loss after mini-batch  4500: 2.30308 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  5000: 2.30398 (of which 0.00010 l2 loss)\n",
            "Loss after mini-batch  5500: 2.30274 (of which 0.00010 l2 loss)\n",
            "Loss after mini-batch  6000: 2.30284 (of which 0.00009 l2 loss)\n",
            "Starting epoch 4\n",
            "Loss after mini-batch   500: 2.30105 (of which 0.00010 l2 loss)\n",
            "Loss after mini-batch  1000: 2.30233 (of which 0.00009 l2 loss)\n",
            "Loss after mini-batch  1500: 2.30122 (of which 0.00012 l2 loss)\n",
            "Loss after mini-batch  2000: 2.30151 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  2500: 2.30151 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  3000: 2.30214 (of which 0.00011 l2 loss)\n",
            "Loss after mini-batch  3500: 2.30338 (of which 0.00008 l2 loss)\n",
            "Loss after mini-batch  4000: 2.30284 (of which 0.00008 l2 loss)\n",
            "Loss after mini-batch  4500: 2.30178 (of which 0.00011 l2 loss)\n",
            "Loss after mini-batch  5000: 2.30343 (of which 0.00006 l2 loss)\n",
            "Loss after mini-batch  5500: 2.30285 (of which 0.00008 l2 loss)\n",
            "Loss after mini-batch  6000: 2.30251 (of which 0.00004 l2 loss)\n",
            "Starting epoch 5\n",
            "Loss after mini-batch   500: 2.30324 (of which 0.00011 l2 loss)\n",
            "Loss after mini-batch  1000: 2.30488 (of which 0.00011 l2 loss)\n",
            "Loss after mini-batch  1500: 2.30274 (of which 0.00004 l2 loss)\n",
            "Loss after mini-batch  2000: 2.30191 (of which 0.00004 l2 loss)\n",
            "Loss after mini-batch  2500: 2.30102 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  3000: 2.30194 (of which 0.00008 l2 loss)\n",
            "Loss after mini-batch  3500: 2.30228 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  4000: 2.30383 (of which 0.00009 l2 loss)\n",
            "Loss after mini-batch  4500: 2.30140 (of which 0.00006 l2 loss)\n",
            "Loss after mini-batch  5000: 2.30330 (of which 0.00009 l2 loss)\n",
            "Loss after mini-batch  5500: 2.30312 (of which 0.00007 l2 loss)\n",
            "Loss after mini-batch  6000: 2.30295 (of which 0.00011 l2 loss)\n",
            "Training process has finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transfer Learning\n",
        "\n",
        "#I did this whole assignmet using transfer learning...."
      ],
      "metadata": {
        "id": "Vd43W4UqVNX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "juFNcuX-VNb3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}